# -*- coding: utf-8 -*-
"""Khóa luận 1 - 15520214 - 15520452.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pl4cYm5Hg57EWgOOalNttfcZGBDdyerr

Sinh viên thực hiện:
 - Nguyễn Hoàng Hiệp - 15520214
 - Nguyễn Hoàng Luân - 15520452

##1.Khai báo các thư viện cần thiết
"""

# Commented out IPython magic to ensure Python compatibility.
# ***
# Import library
# Thư viện sử lý số
import pandas as pd
import numpy as np
import time
from collections import Counter
import matplotlib.patches as mpatches
from timeit import default_timer as timer

# Thư hiện vẽ biểu đồ 
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
from matplotlib.legend_handler import HandlerLine2D
import seaborn as sns
import itertools

# Thư viện xây dựng mô hình
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold
from imblearn.over_sampling import RandomOverSampler
from imblearn.combine import SMOTEENN

# Classifier Libraries
from sklearn.ensemble import RandomForestClassifier
from xgboost.sklearn import XGBClassifier

import warnings
warnings.filterwarnings("ignore")

"""## Connect to google drive

### Cách 1: Sử dụng dữ liệu cá nhân
"""

# Kết nối với gg driver
from google.colab import drive
drive.mount('drive')
# Cách pass auth: Nhấn vào link -> chọn tài khoản google uit -> copy auth -> dán vào ô input bên dưới-> nhấn enter

# Đường dẫn đến nơi lưu trữ dữ liệu trong drive
path = '/content/drive/My Drive/Colab Notebooks/Graduation Thesis/\
Data/data 1.csv'
print('DONE')

"""### Cách 2: có thể chia sẻ cho người khác có thể chạy thử"""

# ***
# Import PyDrive and associated libraries.
# This only needs to be done once per notebook.
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# ***
# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# ***
# DỮ LIỆU THÔ
# Download a file based on its file ID.
#  là id đường dẫn chia sẽ file tài liều
# example: https://drive.google.com/file/d/1rNwi9Ki1ujzHH4Q0rzXImW47tbZmR3Ds/view
# ID : 1rNwi9Ki1ujzHH4Q0rzXImW47tbZmR3Ds
file_id = '1iGVOvK2Td5qZdRkv76cMeMGEWX-0P_aN'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('financial.csv') # dowload file
# Đọc dữ liệu
data = pd.read_csv('financial.csv')

# ============ Chạy model - chỉ cần gọi tệp dữ liệu đã được làm sạch ===========
# DỮ LIỆU ĐÃ LÀM SẠCH VÀ CHƯA SỬ LÝ CÂN BẰNG
# Link dữ liệu: https://drive.google.com/file/d/1jzYF3YPckd619VnNMqeHW-xuiF7lqzSH/view?usp=sharing
file_id = '1jzYF3YPckd619VnNMqeHW-xuiF7lqzSH'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('data_train.csv') # dowload file
data_clean = pd.read_csv('data_train.csv')

# DỮ LIỆU ĐÃ LÀM SẠCH VÀ SỬ LÝ CÂN BẰNG: UNDERSAMPLING
# Link dữ liệu: https://drive.google.com/file/d/1-JBcZ6YPCCKeei0iZ0zPcgFKGVm8-IP3/view?usp=sharing
file_id = '1-JBcZ6YPCCKeei0iZ0zPcgFKGVm8-IP3'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('data_under_sampling.csv') # dowload file
data_under_sampling = pd.read_csv('data_under_sampling.csv')

# DỮ LIỆU ĐÃ LÀM SẠCH VÀ SỬ LÝ CÂN BẰNG: OVERSAMPLING
# Link dữ liệu: https://drive.google.com/file/d/1Ris1CnorA4uHGAIjPrOdu2N0GcKaPwI6/view?usp=sharing
file_id = '1Ris1CnorA4uHGAIjPrOdu2N0GcKaPwI6'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('data_over_sampling.csv') # dowload file
df_with_overspampling = pd.read_csv('data_over_sampling.csv')

"""##2.Tìm hiểu tổng quan dữ liệu

Mô tả:
PaySim mô phỏng các giao dịch tiền điện thoại di động dựa trên một mẫu các giao dịch thực được trích từ một tháng nhật ký tài chính từ một dịch vụ tiền điện thoại di động được thực hiện ở một quốc gia châu Phi. Nhật ký ban đầu được cung cấp bởi một công ty đa quốc gia, nhà cung cấp dịch vụ tài chính di động hiện đang hoạt động tại hơn 14 quốc gia trên toàn thế giới.

**Headers - mô tả được cung cấp**  

1,PAYMENT,1060.31,C429214117,1089.0,28.69,M1591654462,0.0,0.0,0,0

- step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).

- type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.

- amount - amount of the transaction in local currency.

- nameOrig - customer who started the transaction

- oldbalanceOrg - initial balance before the transaction

- newbalanceOrig - new balance after the transaction

- nameDest - customer who is the recipient of the transaction

- oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).

- newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).

- isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.

- isFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.

###Kiểu dữ liệu các thuộc tính
"""

data.head()

# Hiện thị kiểu dữ liểu của các thuộc tính
data.dtypes

data.describe()

"""###Kích thước dữ liệu"""

# Kích thước dữ liệu
data.shape

"""**Nhận xét:**
- Dữ liệu có 6.362.620 dòng, 11 cột như trên.
"""

# Số lượng giao dịch bình thường và số lượng giao dịch gian lận
data.isFraud.value_counts()

"""**Nhận xét:**
- Dữ liệu có 6354407 giao dịch bình thường
- và 8213 giao dịch gian lận

###Đếm số dòng dữ liệu bị thiếu
"""

data.isnull().values.any()

# BƯớc này không cần thiết trong trường hợp này vì không có dữ liệu khuyết
missing_data = data.isnull()
missing_data.head(5)

"""- True: dữ liệu bị thiếu
- False: có dữ liệu

###Đếm số dòng dữ liệu thiếu cho mỗi thuộc tính
"""

# BƯớc này không cần thiết trong trường hợp này vì không có dữ liệu khuyết
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")

"""==> Không có dữ liệu thiếu trong tập dữ liệu

##3.Phân tích dữ liệu

###3.1 những loại giao dịch gian lận

####Phân phối xác suất cột loại giao dịch
"""

# Cột loại giao dịch
data['type'].value_counts().to_frame()

# Biểu đồ thể hiện sô lượng theo loại giao dịch
f, ax = plt.subplots(1, 1, figsize=(8, 8))
data.type.value_counts().plot(kind='bar', title="Loại giao dịch", ax=ax, figsize=(8,8))
plt.show()

# Phân phối xác suất giao dịch gian lận là không gian lận ở giao dịch 'PAYMENT'
F = data[data['type'] == 'PAYMENT']['isFraud']
print('PAYMENT {}'.format(Counter(F)))

"""Ở cột `isFraud` Trong tập dữ liệu:  
    - 0: là giao dịch bình thường
    - 1: là giao dịch gian lận  
==> Toàn bộ giao dịch loại `PAYMENT` là giao dich bình thường

Thực hiện tương tự cho các loại giao dịch còn lại:
"""

F = data[data['type'] == 'TRANSFER']['isFraud']
print('TRANSFER {}'.format(Counter(F)))
F = data[data['type'] == 'CASH_OUT']['isFraud']
print('CASH_OUT {}'.format(Counter(F)))
F = data[data['type'] == 'DEBIT']['isFraud']
print('DEBIT {}'.format(Counter(F)))
F = data[data['type'] == 'CASH_IN']['isFraud']
print('CASH_IN {}'.format(Counter(F)))

# Biểu đồ thể hiện số lượng giao dịch theo loại giao dịch trên dừng loại gian lận
ax = data.groupby(['type', 'isFraud']).size().plot(kind='bar', figsize=(10, 8))
ax.set_title("Số giao dịch bất thường trên mỗi loại giao dịch('isFraud')")
ax.set_xlabel("(Loại giao dịch, bất thường hoặc bình thường)")
ax.set_ylabel("Số lượng giao dịch")
for p in ax.patches:
    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))

data.groupby(['type', 'isFraud']).size()

"""Nhận xét:  
    - Chỉ có 2 loại giao dịch `TRANSFER` và `CASH_OUT` là có giao dịch gian lận    
    - Số lượng giao dịch gian lận ở 2 loại giao dịch gần như bằng nhau  
Một giả định dự đoán có:  
    - Giao dịch gian lận: Tiền được chuyển từ tài khoản này -> sang tài khoảng khác để rút ra
"""

print('\n Các loại giao dịch có giao dịch gian lận {}'.format(\
list(data.loc[data.isFraud == 1].type.drop_duplicates().values)))

dfFraudTransfer = data.loc[(data.isFraud == 1) & (data.type == 'TRANSFER')]
dfFraudCashout = data.loc[(data.isFraud == 1) & (data.type == 'CASH_OUT')]

print ('\n Số lượng \'giao dịch chuyển tiền\' là gian lận = {}'.\
       format(len(dfFraudTransfer)))

print ('\n Số lượng \'giao dịch rút tiền\' là gian lận = {}'.\
       format(len(dfFraudCashout))) # 4116

"""###3.2 Tìm hiểu ý nghĩa thuộc tính isFlaggedFraud

Thông tin được cung cấp: isFlaggedFraud = 1 khi giao dịch chuyển tiền có giá trị lớn hơn 200.000(không chỉ rõ giá trị nào) ==> cần tìm hiểu
"""

ax = data.groupby(['type', 'isFlaggedFraud']).size().plot(kind='bar', figsize=(10, 8))
ax.set_title("Số giao dịch bất thường trên mỗi loại giao dịch('isFlaggedFraud')")
ax.set_xlabel("(Loại giao dịch('type'), Bất thường hoặc bình thường('isFlaggedFraud'))")
ax.set_ylabel("Số lượng giao dịch")
for p in ax.patches:
    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))

data.groupby(['type', 'isFlaggedFraud']).size()

"""**Nhận xét:**
- Chỉ có 16 giao dịch chuyển tiền được đánh dấu 'isFlaggedFraud' trong toàn bộ dữ liệu

####Giả thuyết: Thuộc tính 'amount' >= 200000
"""

print('\nNhững loại giao dịch có thuộc tính `isFlaggedFraud` được đánh dấu: \
{}'.format(list(data.loc[data.isFlaggedFraud == 1].type.drop_duplicates())))

dfTransfer = data.loc[data.type == 'TRANSFER'] # Lấy ra các dòng có type là TRANSFER
dfFlagged = data.loc[data.isFlaggedFraud == 1] # Lấy ra các dòng có isFlaggedFraud là 1
dfNotFlagged = data.loc[data.isFlaggedFraud == 0] # Lấy ra các dòng có isFlaggedFraud là 0

print('\nGiá trị giao dịch nhỏ nhất khi isFlaggedFraud được đánh dấu(1) = {}'\
                                  .format(dfFlagged.amount.min()))

print('\nGiá trị giao dịch lớn nhất có loại giao dịch chuyển tiền khi isFlaggedFraud không được đánh dấu(0)=\
 {}'.format(dfTransfer.loc[dfTransfer.isFlaggedFraud == 0].amount.max()))

dfTransfer.isFlaggedFraud.sum()

len(dfTransfer.loc[(dfTransfer['amount'] >= 200000) & (dfTransfer['isFlaggedFraud'] == 0)])

"""***Thực tế ***  
*   isFlaggedFraud vẫn không thể được đặt mặc dù điều kiện này được đáp ứng.(409094 như kết quả ở trên)  
==>   Loại trường hợp 'isFlaggedFraud' phụ thuộc vào điều kiện thuộc tính 'amount' >= 200000

####Giả thuyết: isFlaggedFraud = 1 khi giao dịch chuyển tiền bị hủy
- Số dư tài khoản nhận tiền trước và sau giao dịch đều bằng 0.
"""

print('\nSố giao dịch chuyển tiền với isFlaggedFraud = 0, oldBalanceDest = 0 và\
 newBalanceDest = 0: {}'.\
format(len(dfTransfer.loc[(dfTransfer.isFlaggedFraud == 0) & \
(dfTransfer.oldbalanceDest == 0) & (dfTransfer.newbalanceDest == 0)])))

"""==> Các điều kiện này cũng không xác định được trạng thái của `isFlaggedFraud`  
Loại giải thuyết này

####Giả thuyết: isFlaggedFraud = 1 khi `oldbalanceOrg` = `newbalanceOrg`
- Không set thuộc tính 'newBalanceOrig' vì nó chỉ được cập nhật sau khi giao dịch, trong khi 'isFlaggedFraud' sẽ được đặt trước khi giao dịch diễn ra
"""

print('Giá trị \'nhỏ nhất\' \'lớn nhất\' của thuộc tính \'oldBalanceOrig\' với \'isFlaggedFraud\' = 1 các giao dịch TRANSFERs: {}'.\
format([round(dfFlagged.oldbalanceOrg.min()), round(dfFlagged.oldbalanceOrg.max())]))

print('\nGiá trị \'nhỏ nhất\' \'lớn nhất\' của thuộc tính \'oldBalanceOrig\' for \'isFlaggedFraud\' = 0 các giao dịch TRANSFERs với điều kiện \
\'oldBalanceOrig\' = \
newbalanceOrig: {}'.format(\
[dfTransfer.loc[(dfTransfer.isFlaggedFraud == 0) & (dfTransfer.oldbalanceOrg \
== dfTransfer.newbalanceOrig)].oldbalanceOrg.min(), \
round(dfTransfer.loc[(dfTransfer.isFlaggedFraud == 0) & (dfTransfer.oldbalanceOrg \
               == dfTransfer.newbalanceOrig)].oldbalanceOrg.max())]))

"""Nhận xét:
- Có tồn tại trường hợp thỏa điều kiện những thuôc tính 'isFlaggedFraud' = 0  
==> Loại giả thuyết này

####Giả thuyết: 'IsFlaggedFraud có thể được đặt dựa trên việc một khách hàng giao dịch nhiều lần không?
"""

print('Người khởi tạo các giao dịch được gắn cờ là gian lận có giao dịch nhiều lần: {}'\
.format((dfFlagged.nameOrig.isin(
    pd.concat(
        [dfNotFlagged.nameOrig,
         dfNotFlagged.nameDest]))).any()))

print('\nCó điểm đến của giao dịch được gắn cờ \'isFlaggedFraud\' là tài khoản đích của các giao dịch khác: \
{}'.format((dfFlagged.nameDest.isin(dfNotFlagged.nameOrig)).any()))

print('\nCó bao nhiêu tài khoản đích của các giao dịch được gắn cờ là gian lận đã là tài khoản đích nhiều lần: {}'\
.format(sum(dfFlagged.nameDest.isin(dfNotFlagged.nameDest))))

"""Nhận xét:  
- Thuộc tính 'isFlaggedFraud' xảy ra ở tất cả các giá trị ở các thuộc tính khác
- 'isFlaggedFraud' dường như không liên quan đến bất kỳ biến hoặc tính năng giải thích nào trong dữ liệu.

Kết Luận:
- Mặc dù 'isFraud' luôn được đặt khi 'isFlaggedFraud' được đặt, vì 'isFlaggedFraud' được đặt chỉ 16 lần theo cách dường như vô nghĩa, chúng tôi  có thể coi tính năng này là không đáng kể và loại bỏ nó trong bộ dữ liệu mà không mất thông tin.

###3.3 Các tài khoản thương nhân được gán nhãn
"""

print('Có tài khoản thương nhân thực hiện giao dịch \'CASH_IN\': {}'.format(\
(data.loc[data.type == 'CASH_IN'].nameOrig.str.contains('M')).any()))

print('\nCó tài khoản thương nhân nào là tài khoản đích trong các thực hiện giao dịch \'CASH_OUT\': {}'.format(\
(data.loc[data.type == 'CASH_OUT'].nameDest.str.contains('M')).any()))

print('Có bất kỳ tài khoàn thương nhân nào là tài khoản đích mà loại giao dịch khác \'PAYMENT\': {}'.format(\
(data.loc[data.nameDest.str.contains('M')].type != 'PAYMENT').any()))

"""Nhận xét:
- Trong thực tế, không có tài khoản thương nhân trong số bất kỳ tài khoản người khởi tạo. Thương nhân chỉ có mặt trong tài khoản đích cho tất cả 'THANH TOÁN'.

####Có nhãn tài khoản phổ biến cho CHUYỂN TIẾP gian lận và CASH_OUT không?

**Theo mô tả được cung cấp**: 
- Phương thức hoạt động để thực hiện hành vi gian lận liên quan đến việc trước tiên thực hiện CHUYỂN GIAO vào tài khoản (lừa đảo), từ đó tiến hành CASH_OUT. CASH_OUT liên quan đến giao dịch với một thương gia thanh toán bằng tiền mặt.
- Do đó, trong quy trình gồm hai bước này, tài khoản gian lận sẽ là cả hai, đích đến trong CHUYỂN GIAO và người khởi tạo trong CASH_OUT. Tuy nhiên, dữ liệu cho thấy bên dưới rằng không có tài khoản phổ biến như vậy trong số các giao dịch gian lận. Do đó, dữ liệu không được in với modus-operandi dự kiến.
"""

# step	type	amount	nameOrig	oldbalanceOrg	newbalanceOrig	nameDest	oldbalanceDest	newbalanceDest	isFraud	isFlaggedFraud
print('Trong các giao dịch gian lận có tài khoản đích của giao dịch chuyển tiền\
 là tài khoản nguồn của giao dịch rút tiền {}'.format(\
(dfFraudTransfer.nameDest.isin(dfFraudCashout.nameOrig)).any())) # False
dfNotFraud = data.loc[data.isFraud == 0]

print('\nTrong các giao dịch gian lận có tài khoản nguồn của giao dịch chuyển tiền\
 là tài khoản nguồn của giao dịch rút tiền: \n')
dfFraudTransfer.loc[dfFraudTransfer.nameDest.\
    isin(dfNotFraud.loc[dfNotFraud.type == 'CASH_OUT'].nameOrig.drop_duplicates())]

print('Giao dịch chuyển tiền đến tài khoản C423543548 vào thời gian = 486 Trong khi \
giao dịch rút tiền từ tài khoản này đã diễn ra rất sớm vào thời gian = ')
dfNotFraud.loc[(dfNotFraud.type == 'CASH_OUT') & (dfNotFraud.nameOrig == 'C423543548')].step.values[0]

"""###Kết luận:
- Từ kết quả thu được ở phần 3.2 và 3.3 ở trên, các thuộc tính 'nameOrig' và 'nameDest' không thể  hiện được tài khoản người bán theo cách dự kiến, Nên chúng thôi bỏ các tính năng này khỏi dữ liệu vì chúng vô nghĩa.

##4.Data cleaning - làm sạch dữ liệu

Từ phân tích dữ liệu sơ bộ dữ liệu của phần 3, chúng tôi biết rằng gian lận chỉ xảy ra trong 'TRANSFER và' CASH_OUT '.  
Vì vậy, chúng tôi chỉ lắp ráp dữ liệu tương ứng để phân tích.  
Dữ liệu mới sẽ được lưu trong biến X
"""

# ***
# Trích ra những dòng dữ liệu là 'chuyển tiền' hoặc 'rút tiền mặt'
X = data.loc[(data.type == 'TRANSFER') | (data.type == 'CASH_OUT')]

# Tách thuộc tính 'phụ thuộc' và 'thuộc tính độc lập'
Y = X['isFraud']
del X['isFraud'] # xóa cột thuộc tính phụ thuộc khỏi X

# Loại bỏ 3 thuộc tính không có nhiều ý nghĩa
X = X.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)

# Chuyển dán nhãn cho loại giao dịch
# TRANSFER => 0
# CASH_OUT => 1
X.loc[X.type == 'TRANSFER', 'type'] = 0
X.loc[X.type == 'CASH_OUT', 'type'] = 1
# Chuyển kiểu dữ kiệu thuộc tính type về là số nguyên
X.type = X.type.astype(int)

# ***
# Xáo trộn dữ liệu (hiện tại đang được sắp xếp theo thuộc tính step)
X = X.loc[np.random.choice(X.index, len(X), replace = False)]
# Dữ liệu sau khi xáo trộn
# X.head(10)
len(X)

# Kiểm tra kết quả các bộ dữ liệu phân tách
X.head(5)

# Tổng quan dữ liệu sau khi lọc dữ liệu
labels = ['Rút tiền', 'Chuyển tiền']
ax = X.type.value_counts().plot(
    kind='bar', title="Transaction type", figsize=(6,6))
for p in ax.patches:
    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.02))
ax.set_xticklabels(labels)
plt.show()

"""###Feature Engineering"""

X = data.loc[(data.type == 'TRANSFER') | (data.type == 'CASH_OUT')]
# Lấy ra các dòng là giao dịch gian lận
Xfraud = X.loc[X.isFraud == 1]
# Lấy ra các dòng là giao dịch không gian lận
XnonFraud = X.loc[X.isFraud == 0]

print('\nTỷ lệ giao dịch gian lận với \'oldBalanceDest\' = \
\'newBalanceDest\' = 0 mặt dù \'amount\' = 0 là: {}'.\
format(len(Xfraud.loc[(Xfraud.oldbalanceDest == 0) & \
(Xfraud.newbalanceDest == 0) & (Xfraud.amount)]) / (1.0 * len(Xfraud))))

print('\nTỉ lệ giao dịch không gian lận với \'oldBalanceDest\' = \
newBalanceDest\' = 0 mặt dù \'amount\' = 0 là: {}'.\
format(len(XnonFraud.loc[(XnonFraud.oldbalanceDest == 0) & \
(XnonFraud.newbalanceDest == 0) & (XnonFraud.amount)]) / (1.0 * len(XnonFraud))))

"""**Nhận xét:**
- Dữ liệu có một số giao dịch với số dư tài khoản đích trước và sau giao dịch đều bằng 0 trong khi sô tiền giao dịch khác 0
- Tỷ của các các giao dịch trên trong dữ liệu:  
    - Với giao dịch gian lận ~ 50%
    - Với giao dịch không gian lận ~ 0.06%

**Kết luận :**
- Có thể nhận thấy số dư tài khoảng đích = 0 là một dấu hiệu mạnh mẽ để phát hiện gian lận. Và việc để = 0 làm cho các thuật toán học máy không hoạt động tốt
=> Thay thế 0 bằng -1 sẽ tốt hơn cho các thuật toán học máy
"""

# ***
X.loc[(X.oldbalanceDest == 0) & (X.newbalanceDest == 0) & (X.amount != 0), \
      ['oldbalanceDest', 'newbalanceDest']] = -1
print('DONE')

"""- Tương tự, dữ liệu cũng có một số giao dịch với số dư bằng 0 trong tài khoản gốc cả trước và sau trong khi số tiền giao dịch là khác không. 
- Trong trường hợp này, tỷ lệ của các giao dịch như vậy nhỏ hơn nhiều so với gian lận (0,3%) so với giao dịch chính hãng (47%).
"""

# ***
X.loc[(X.oldbalanceOrg == 0) & (X.newbalanceOrig == 0) & (X.amount != 0), \
      ['oldbalanceOrg', 'newbalanceOrig']] = -1 #np.nan
print('DONE')

"""####**Tạo 2 thuộc tính mới dựa trên 5 thuộc tính đã có**"""

# ***
# Tạo 2 thuộc tính mới dựa trên 5 thuộc tính đã có
X['errorBalanceOrig'] = X.newbalanceOrig + X.amount - X.oldbalanceOrg # khác 0 là bất bình thường
X['errorBalanceDest'] = X.oldbalanceDest + X.amount - X.newbalanceDest # khác 0 là bất bình thường

X.head()

len(X)

"""Các tính năng mới này hóa ra rất quan trọng trong việc đạt được hiệu suất tốt từ ​​thuật toán ML mà chúng ta sẽ sử dụng. Bảng tương quan giữa các thuộc tính ở bên dưới sẽ chứng minh điều này.

###=>Lưu dữ liệu đã được làm sạch
"""

from google.colab import drive
drive.mount('drive')

# Dự liệu sau khi làm sạch, chưa xử lý mất cân bằng, dùng để train.
X.to_csv('/content/drive/My Drive/Colab Notebooks/Graduation Thesis/Data/data_raw_train_2.csv')

data.head()

"""##5.Trực quan hóa dữ liệu - Data visualization

Cách tốt nhất để xác nhận rằng dữ liệu chứa đủ thông tin để thuật toán ML có thể đưa ra dự đoán mạnh mẽ, là thử và trực tiếp hình dung sự khác biệt giữa các giao dịch gian lận và giao dịch thật.
"""

# Số lượng dòng dữ liệu
# Mục địch lấy chiều dài dữ liệu ra 1 biến sử dụng nhiều lần -> tiết kiệm bộ nhớ
limit = len(X)

# Function vẽ biểu đồ plot
# Tham số 1: Dữ liệu cho cột hoành độ
# Tham số 2: Dữ liệu cho cột tung độ
# Tham số 3: Loại của dòng dữ liệu
# Tham số 4: Kích thước biểu đồ
def plotStrip(x, y, hue, figsize = (14, 9)):
    
    fig = plt.figure(figsize = figsize)
    colours = plt.cm.tab10(np.linspace(0, 1, 9))
    with sns.axes_style('ticks'):
        ax = sns.stripplot(x, y, \
             hue = hue, jitter = 0.4, marker = '.', \
             size = 4, palette = colours)
        ax.set_xlabel('')
        ax.set_xticklabels(['Giao dịch bình thường', 'Giao dịch bất thường'], size = 16)
        for axis in ['top','bottom','left','right']:
            ax.spines[axis].set_linewidth(2)

        handles, labels = ax.get_legend_handles_labels()
        plt.legend(handles, ['Chuyển tiền', 'Rút tiền'], bbox_to_anchor=(1, 1), \
               loc=2, borderaxespad=0, fontsize = 16);
    return ax

"""###5.1 Phân tán theo thời gian"""

# Cột hoành: phân theo loại giao dịch có giận hay giao dịch bình thường
# Cột tung: phân theo giời gian trong ngày
ax = plotStrip(Y[:limit], X.step[:limit], X.type[:limit])
ax.set_ylabel('Thời gian [giờ]', size = 16)
ax.set_title('', size = 20);

"""**Nhận xét:**  
- Dữ liệu không gian lận:
    - Rút tiền vượt trội hơn chuyển tiền
    - Phân phối không đều theo thời gian
- Dữ liệu gian lận:
    - Phân phối đều theo thời gian
    - Số lượng chuyển và rút tiền không chênh lệch nhiều

###5.2 Phân tán theo số tiền giao dịch
"""

# limit = len(X)
ax = plotStrip(Y[:limit], X.amount[:limit], X.type[:limit], figsize = (14, 9))
ax.set_ylabel('amount', size = 16)
ax.set_title('', size = 18);

"""###5.3 Phân tán theo số tiền sai trong tài khoản đích"""

ax = plotStrip(Y[:limit], -X.errorBalanceDest[:limit], X.type[:limit], figsize = (14, 9))
ax.set_ylabel('amount', size = 16)
ax.set_title('', size = 18);

"""**Nhận xét:**
- Với giao dịch không gian lận:
    - Thuộc tính 'amount' có mối tương quan lớn với thuộc tính 'errorBalanceOrig'

##6.Xử lý dữ liệu mất cân bằng
"""

X = data_clean

X.head()

# ***
# Tách dữ liệu giao dịch bình thường và giao dịch bất thường
Xfraud = X.loc[X.isFraud == 1]
# Lấy ra các dòng là giao dịch không gian lận
XnonFraud = X.loc[X.isFraud == 0]
XnonFraud = XnonFraud.sample(frac=1, random_state=42)

print('Số giao dịch bất thường', len(Xfraud))
# Tỷ lệ giao dịch bất thường trên tổng số lượng sao dịch
print('Mất cân bằng = {}%'.format(
    round(len(Xfraud) / float(len(X)), 5)*100))

"""Có thể thấy, sau khi loại bỏ các giao dịch không liên quan và chỉ giữ lại những giao dịch có gian lận. Chỉ có tỉ lệ gian lận thực tế là ~0,3%.
=> Đây là dữ liệu rất mất cân bằng.

Sử lý dữ liệu mất cân bằng theo các phương pháp sau:
- Oversampling
- Undersampling

###6.1 Random UnderSampling

- Trong giai đoạn này của dự án, chúng tôi sẽ triển khai "Lấy mẫu ngẫu nhiên", về cơ bản bao gồm xóa dữ liệu để có bộ dữ liệu cân bằng hơn và do đó tránh các mô hình của chúng tôi bị quá phù hợp với giao dịch bình thường.

- Nhược điểm của việc lấy mẫu dưới là một mô hình được đào tạo theo cách này sẽ không hoạt động tốt trên dữ liệu kiểm tra sai lệch trong thế giới thực vì hầu như tất cả các thông tin đã bị loại bỏ.

Các bước thực hiện:
- Đầu tiên chúng ta phải làm là xác định mức độ mất cân bằng của lớp chúng ta.
- Đưa số lượng mẫu không gian lận bằng với số lượng giao dịch gian lận(50/50) => cụ thể là 8213 giao dịch gian lận là 8213 giao dịch không gian lận.
- Sau khi đã có tập dữ liệu, chúng tôi xáo trộn dữ liệu, mục đích để xem liệu mô hình của chúng tôi có duy trì được độ chính xác ổn định hay không.
"""

# ***
# lấy ra 8213 giao dịch bình thường
non_fraud_df = XnonFraud[:8213]

# Gộp dữ liệu bất thường là bình thường
normal_distributed_df = pd.concat([Xfraud, non_fraud_df])

# xáo trộn dữ liệu
df_with_underspampling = normal_distributed_df.sample(frac=1, random_state=42)
df_with_underspampling = df_with_underspampling.iloc[:,1:]

print('Số giao dịch còn lại: {}'.format(len(df_with_underspampling)))
df_with_underspampling.head()

"""**Nhận xét:**
- Phương pháp lấy mẫu ngẫu nhiên có thể xử lý dữ liệu mất cân bằng. Tuy nhiên nó cũng mang lại nhiều rủi ro cho mô hình của chúng tôi vì có rất nhiều thông tin bị mất trong dữ liệu lượt bỏ (information loss).

**Bây giờ chúng ta đã cân bằng chính xác tập dữ liệu của mình, chúng ta có thể bắc đầu với phân tích và tiền xử lý dữ liệu trên dữ liệu mới.**
"""

print('Phân phối các lớp trong tập dữ liệu mẫu')
print(df_with_underspampling['isFraud'].value_counts()/len(df_with_underspampling))

sns.countplot('isFraud', data=df_with_underspampling)
plt.title('Biểu đố phân tán', fontsize=14)
plt.xlabel('Bất thường')
plt.ylabel('Số giao dịch')
plt.show()

"""###6.2 Random OverSampling

**Oversampling**: làm tăng trọng số của lớp thiểu số bằng cách sao chép các ví dụ của lớp thiểu số.  Mặc dù nó không làm tăng thông tin, nhưng nó làm tăng vấn đề quá khớp, khiến cho mô hình quá cụ thể.  Nó cũng có thể là trường hợp độ chính xác cho tập huấn luyện là cao, nhưng hiệu suất cho các bộ dữ liệu mới thực sự kém hơn. 

**Quá khổ ngẫu nhiên (random oversampling)**
- Quá khổ ngẫu nhiên đơn giản là sao chép ngẫu nhiên các ví dụ lớp thiểu số.  
- Quá khổ ngẫu nhiên được biết là làm tăng khả năng xảy ra quá mức.
- Mặt khác, nhược điểm lớn của việc lấy mẫu ngẫu nhiên là phương pháp này có thể loại bỏ dữ liệu hữu ích.
"""

# ***
# Dữ liệu đã làm sạch: data_clean
# Tách dữ liệu giao dịch bình thường và giao dịch bất thường
y_rs = data_clean['isFraud']
# Lấy ra các dòng là giao dịch bình thường
X_rs = data_clean.drop(['isFraud'], axis = 1)
name_x = X_rs.columns

# RandomOverSampler: reference https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html
sampler = RandomOverSampler(random_state=42)
X_rs, y_rs = sampler.fit_sample(X_rs, y_rs)
print('RandomOverSampler {}'.format(Counter(y_rs)))

# Chuyển dữ liệu ndarray to DataFrame
X_rs = pd.DataFrame(X_rs)
y_rs = pd.DataFrame(y_rs)
X_rs = X_rs.iloc[:,1:]
X_rs.columns = name_x[1:]
y_rs.columns = ['isFraud']

# Gộp dữ liệu bất thường và bình thường
normal_distributed_df = pd.concat([X_rs, y_rs], axis=1)
# xáo trộn dữ liệu
df_with_overspampling = normal_distributed_df.sample(frac=1, random_state=42)

df_with_overspampling.head(5)

print('Phân phối các lớp trong tập dữ liệu mẫu')
print(df_with_overspampling['isFraud'].value_counts()/len(df_with_overspampling))

sns.countplot('isFraud', data=df_with_overspampling)
plt.title('Biểu đố phân tán', fontsize=14)
plt.xlabel('Bất thường')
plt.ylabel('Số giao dịch')
plt.show()

"""###6.3 Ma trận tương quan(Correlation Matrices)

- Chúng tôi muốn biết liệu có những tính năng có ảnh hưởng lớn đến việc một giao dịch cụ thể có phải là lừa đảo hay không. Tuy nhiên, điều quan trọng là chúng tôi sử dụng đúng khung dữ liệu (tập mẫu) để chúng tôi xem các tính năng nào có mối tương quan tích cực hoặc tiêu cực cao liên quan đến các giao dịch gian lận.
- Ma trận tương quan là một ma trận thể hiện các hệ số tương quan giữa các biến. Mỗi ô trong bảng hiển thị mối tương quan giữa hai biến. 
- Một ma trận tương quan được sử dụng để tóm tắt dữ liệu, làm đầu vào cho một phân tích nâng cao hơn và như một chẩn đoán cho các phân tích nâng cao.

#### 6.3.1 Dữ liệu chưa xử lý cân bằng
"""

# Ma trận tương quan với dữ liệu mất cân bằng(dữ liệu gốc)
# calculate the correlation matrix
corr = data_clean.corr()
# plot the heatmap
plt.figure(figsize=(16,12))
ax = sns.heatmap(corr, annot = True,
        xticklabels=corr.columns,
        yticklabels=corr.columns, cmap='Blues', annot_kws={'size':20})
plt.show()

"""#### 6.3.2 Dữ liệu cân bằng theo phương pháp UnderSampling"""

# Ma trận tương quan với dữ liệu cân bằng theo phương pháp UnderSampling
# calculate the correlation matrix
corr = df_with_underspampling.corr()
# plot the heatmap
plt.figure(figsize=(16,12))
ax = sns.heatmap(corr, annot = True,
        xticklabels=corr.columns,
        yticklabels=corr.columns, cmap='Blues', annot_kws={'size':20})
plt.show()

"""**Nhận xét:**

- Tương quan âm: có thể thấy thuôc tính 'errorBalanceOrig', 'newbalanceDest' có tượng quan ngược cao nhất với biến phân loại của chúng ta.
- Lưu ý rằng các giá trị này càng thấp thì kết quả cuối cùng sẽ là một giao dịch gian lận.
- BoxPlots: Chúng tôi sẽ sử dụng các biểu đồ boxplots để hiểu rõ hơn về việc phân phối các tính năng này trong các giao dịch gian lận và không gian lận.

#### 6.3.3 Dữ liệu cân bằng theo phương pháp OverSampling
"""

# Ma trận tương quan với dữ liệu cân bằng theo phương pháp OverSampling
# calculate the correlation matrix
corr = df_with_overspampling.corr()
# plot the heatmap
plt.figure(figsize=(16,12))
ax = sns.heatmap(corr, annot = True,
        xticklabels=corr.columns,
        yticklabels=corr.columns, cmap='Blues', annot_kws={'size':20})
plt.show()

"""###6.4 Phát hiện bất thường - Loại bỏ ngoại lệ(Anomaly Detection)

- Mục đích chính của chúng tôi trong phần này là loại bỏ "các dữ liệu ngoại vị" khỏi các tính năng có mối tương quan cao với các lớp của chúng tôi.
- Điều này sẽ có tác động tích cực đến độ chính xác của các mô hình của chúng tôi.

- Chúng ta phải cẩn thận về ngưỡng mà chúng ta muốn loại bỏ để loại bỏ các ngoại lệ. 
- Chúng tôi xác định ngưỡng bằng cách nhân một số (ví dụ: 1,5) với (Phạm vi liên dải). Ngưỡng này càng cao, càng ít ngoại lệ sẽ phát hiện (nhân với số cao hơn ex: 3) và ngưỡng này càng thấp thì càng phát hiện ra nhiều ngoại lệ.

**Sự cân bằng:** 
- chúng tôi muốn tập trung nhiều hơn vào "các ngoại lệ cực đoan" thay vì chỉ các ngoại lệ. 
- Bởi vì chúng tôi có thể gặp rủi ro mất thông tin, điều này sẽ khiến các mô hình của chúng tôi có độ chính xác thấp hơn. 
- Xem 'ngưỡng'(threshold) nó ảnh hưởng đến độ chính xác của các mô hình phân loại của chúng tôi như thế nào.

####6.4.1 Phân phối trước khi loại bỏ giá trị ngoại vi
"""

# ***
# UNDERSAMPLING
f, axes = plt.subplots(ncols=4, figsize=(20,15))

# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.boxplot(x="isFraud", y="errorBalanceOrig", data=df_with_underspampling, ax=axes[0], palette="Set2")
axes[0].set_title('errorBalanceOrig vs isFraud')

sns.boxplot(x="isFraud", y="errorBalanceDest", data=df_with_underspampling, ax=axes[1], palette="Set2")
axes[1].set_title('errorBalanceDest vs isFraud')

sns.boxplot(x="isFraud", y="oldbalanceDest", data=df_with_underspampling, ax=axes[2], palette="Set2")
axes[2].set_title('oldbalanceDest vs isFraud')

sns.boxplot(x="isFraud", y="newbalanceDest", data=df_with_underspampling, ax=axes[3], palette="Set2")
axes[3].set_title('newbalanceDest vs isFraud')

plt.show()

# ***
# OVERSAMPLING
f, axes = plt.subplots(ncols=4, figsize=(20,15))

sns.boxplot(x="isFraud", y="errorBalanceOrig", data=df_with_overspampling, ax=axes[0], palette="Set2")
axes[0].set_title('errorBalanceOrig vs isFraud')

sns.boxplot(x="isFraud", y="errorBalanceDest", data=df_with_overspampling, ax=axes[1], palette="Set2")
axes[1].set_title('errorBalanceDest vs isFraud')

sns.boxplot(x="isFraud", y="oldbalanceDest", data=df_with_overspampling, ax=axes[2], palette="Set2")
axes[2].set_title('oldbalanceDest vs isFraud')

sns.boxplot(x="isFraud", y="newbalanceDest", data=df_with_overspampling, ax=axes[3], palette="Set2")
axes[3].set_title('newbalanceDest vs isFraud')

plt.show()

"""####6.4.2 Loại bỏ giá trị ngoại vi

#####6.4.2.1 UNDERSAMPLING
"""

# ***
errorBalanceOrig_fraud = df_with_underspampling['errorBalanceOrig'].loc[df_with_underspampling['isFraud'] == 1].values
# Bách phân vị 25%, 75%
q25, q75 = np.percentile(errorBalanceOrig_fraud, 25), np.percentile(errorBalanceOrig_fraud, 75)
print('Bách phân vị 25: {} | Bách phân vị 75: {}'.format(q25, q75))
# Phạm vi liên vùng
errorBalanceOrig_iqr = q75 - q25
print('iqr: {}'.format(errorBalanceOrig_iqr))

# Tham số ngưỡng chúng tôi chọn là '1.5'
errorBalanceOrig_cut_off = errorBalanceOrig_iqr * 1.5
# Giá trị biên dưới, biên trên
errorBalanceOrig_lower, errorBalanceOrig_upper = q25 - errorBalanceOrig_cut_off, q75 + errorBalanceOrig_cut_off
# print('Các giá trị ngoại vi: {}'.format(errorBalanceOrig_cut_off))
print('Biên dưới: {}'.format(errorBalanceOrig_lower))
print('Biên trên: {}'.format(errorBalanceOrig_upper))

# Dữ liệu ngoại vi
outliers = [x for x in errorBalanceOrig_fraud if x < errorBalanceOrig_lower or x > errorBalanceOrig_upper]
print('Tính năng \'errorBalanceOrig\' có giá trị ngoại vi cho trường hợp là giao dịch gian lận: {}'.format(len(outliers)))
print('Giá trị ngoại vi:{}'.format(outliers))
print('Số lượng giá trị ngoại vi:{}'.format(len(outliers)))
# Loại bỏ dữ liệu là ngoại vi
df_with_underspampling = df_with_underspampling.drop(df_with_underspampling[(df_with_underspampling['errorBalanceOrig'] > errorBalanceOrig_upper) | (df_with_underspampling['errorBalanceOrig'] < errorBalanceOrig_lower)].index)
print('----' * 44)

# ***
# -----> Thực hiện tương tự
errorBalanceDest_fraud = new_df['errorBalanceDest'].loc[new_df['isFraud'] == 1].values
q25, q75 = np.percentile(errorBalanceDest_fraud, 25), np.percentile(errorBalanceDest_fraud, 75)
errorBalanceDest_iqr = q75 - q25

errorBalanceDest_cut_off = errorBalanceDest_iqr * 1.5
errorBalanceDest_lower, errorBalanceDest_upper = q25 - errorBalanceDest_cut_off, q75 + errorBalanceDest_cut_off
print('Biên dưới: {}'.format(errorBalanceDest_lower))
print('Biên trên: {}'.format(errorBalanceDest_upper))
outliers = [x for x in errorBalanceDest_fraud if x < errorBalanceDest_lower or x > errorBalanceDest_upper]
print('Giá trị ngoại vi: {}'.format(outliers))
print('Tính năng \'errorBalanceDest\' có giá trị ngoại vi cho trường hợp là giao dịch gian lận: {}'.format(len(outliers)))
new_df = new_df.drop(new_df[(new_df['errorBalanceDest'] > errorBalanceDest_upper) | (new_df['errorBalanceDest'] < errorBalanceDest_lower)].index)
print('Số giao dịch còn lại sau khi lượt bỏ: {}'.format(len(new_df)))
print('----' * 44)


# Với thuộc tính 'oldbalanceDest'
oldbalanceDest_fraud = new_df['oldbalanceDest'].loc[new_df['isFraud'] == 1].values
q25, q75 = np.percentile(oldbalanceDest_fraud, 25), np.percentile(oldbalanceDest_fraud, 75)

oldbalanceDest_iqr = q75 - q25

oldbalanceDest_cut_off = oldbalanceDest_iqr * 1.5
oldbalanceDest_lower, oldbalanceDest_upper = q25 - oldbalanceDest_cut_off, q75 + oldbalanceDest_cut_off
print('Biên dưới: {}'.format(oldbalanceDest_lower))
print('Biên trên: {}'.format(oldbalanceDest_upper))
outliers = [x for x in oldbalanceDest_fraud if x < oldbalanceDest_lower or x > oldbalanceDest_upper]
print('oldbalanceDest outliers: {}'.format(outliers))
print('Tính năng \'oldbalanceDest\' có giá trị ngoại vi cho trường hợp là giao dịch gian lận: {}'.format(len(outliers)))
new_df = new_df.drop(new_df[(new_df['oldbalanceDest'] > oldbalanceDest_upper) | (new_df['oldbalanceDest'] < oldbalanceDest_lower)].index)
print('Số giao dịch còn lại sau khi lượt bỏ: {}'.format(len(new_df)))

"""#####6.4.2.1 OVERSAMPLING"""

# ***
errorBalanceOrig_fraud = df_with_overspampling['errorBalanceOrig'].loc[df_with_overspampling['isFraud'] == 1].values
# Bách phân vị 25%, 75%
q25, q75 = np.percentile(errorBalanceOrig_fraud, 25), np.percentile(errorBalanceOrig_fraud, 75)
print('Bách phân vị 25: {} | Bách phân vị 75: {}'.format(q25, q75))
# Phạm vi liên vùng
errorBalanceOrig_iqr = q75 - q25
print('iqr: {}'.format(errorBalanceOrig_iqr))

# Tham số ngưỡng chúng tôi chọn là '1.5'
errorBalanceOrig_cut_off = errorBalanceOrig_iqr * 1.5
# Giá trị biên dưới, biên trên
errorBalanceOrig_lower, errorBalanceOrig_upper = q25 - errorBalanceOrig_cut_off, q75 + errorBalanceOrig_cut_off
# print('Các giá trị ngoại vi: {}'.format(errorBalanceOrig_cut_off))
print('Biên dưới: {}'.format(errorBalanceOrig_lower))
print('Biên trên: {}'.format(errorBalanceOrig_upper))

# Dữ liệu ngoại vi
outliers = [x for x in errorBalanceOrig_fraud if x < errorBalanceOrig_lower or x > errorBalanceOrig_upper]
print('Tính năng \'errorBalanceOrig\' có giá trị ngoại vi cho trường hợp là giao dịch gian lận: {}'.format(len(outliers)))
# print('Giá trị ngoại vi:{}'.format(outliers))
# Loại bỏ dữ liệu là ngoại vi
df_with_overspampling = df_with_overspampling.drop(
    df_with_overspampling[(
        df_with_overspampling['errorBalanceOrig'] > errorBalanceOrig_upper) | (df_with_overspampling['errorBalanceOrig'] < errorBalanceOrig_lower)].index)
print('----' * 44)

# ***
# -----> Thực hiện tương tự
errorBalanceDest_fraud = df_with_overspampling['errorBalanceDest'].loc[df_with_overspampling['isFraud'] == 1].values
q25, q75 = np.percentile(errorBalanceDest_fraud, 25), np.percentile(errorBalanceDest_fraud, 75)
errorBalanceDest_iqr = q75 - q25

errorBalanceDest_cut_off = errorBalanceDest_iqr * 1.5
errorBalanceDest_lower, errorBalanceDest_upper = q25 - errorBalanceDest_cut_off, q75 + errorBalanceDest_cut_off
print('Biên dưới: {}'.format(errorBalanceDest_lower))
print('Biên trên: {}'.format(errorBalanceDest_upper))
outliers = [x for x in errorBalanceDest_fraud if x < errorBalanceDest_lower or x > errorBalanceDest_upper]
print('Tính năng \'errorBalanceDest\' có giá trị ngoại vi cho trường hợp là giao dịch gian lận: {}'.format(len(outliers)))
df_with_overspampling = df_with_overspampling.drop(df_with_overspampling[(
    df_with_overspampling['errorBalanceDest'] > errorBalanceDest_upper) | (df_with_overspampling['errorBalanceDest'] < errorBalanceDest_lower)].index)
print('Số giao dịch còn lại sau khi lượt bỏ: {}'.format(len(df_with_overspampling)))
print('----' * 44)


# Với thuộc tính 'oldbalanceDest'
oldbalanceDest_fraud = df_with_overspampling['oldbalanceDest'].loc[df_with_overspampling['isFraud'] == 1].values
q25, q75 = np.percentile(oldbalanceDest_fraud, 25), np.percentile(oldbalanceDest_fraud, 75)
oldbalanceDest_iqr = q75 - q25

oldbalanceDest_cut_off = oldbalanceDest_iqr * 1.5
oldbalanceDest_lower, oldbalanceDest_upper = q25 - oldbalanceDest_cut_off, q75 + oldbalanceDest_cut_off
print('Biên dưới: {}'.format(oldbalanceDest_lower))
print('Biên trên: {}'.format(oldbalanceDest_upper))
outliers = [x for x in oldbalanceDest_fraud if x < oldbalanceDest_lower or x > oldbalanceDest_upper]
print('Tính năng \'oldbalanceDest\' có giá trị ngoại vi cho trường hợp là giao dịch gian lận: {}'.format(len(outliers)))
df_with_overspampling = df_with_overspampling.drop(df_with_overspampling[(
    df_with_overspampling['oldbalanceDest'] > oldbalanceDest_upper) | (df_with_overspampling['oldbalanceDest'] < oldbalanceDest_lower)].index)
print('Số giao dịch còn lại sau khi lượt bỏ: {}'.format(len(df_with_overspampling)))

"""####6.3.2 Phân phối sau phi bỏ giá trị ngoại vi

#####6.5.3.1 UNDERSAMPLING
"""

f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))

colors = ['#B3F9C5', '#f9c5b3']
# Boxplots với dữ liệu không dữ liệu ngoại vi đã được lượt bỏ
# Feature errorBalanceOrig
sns.boxplot(x="isFraud", y="errorBalanceOrig", data=new_df,ax=ax1, palette=colors)
ax1.set_title("errorBalanceOrig Feature \n Reduction of outliers", fontsize=14)

# # Feature errorBalanceDest
sns.boxplot(x="isFraud", y="errorBalanceDest", data=new_df, ax=ax2, palette=colors)
ax2.set_title("errorBalanceDest Feature \n Reduction of outliers", fontsize=14)

# # Feature oldbalanceDest
sns.boxplot(x="isFraud", y="oldbalanceDest", data=new_df, ax=ax3, palette=colors)
ax3.set_title("oldbalanceDest Feature \n Reduction of outliers", fontsize=14)

plt.show()

"""#####6.5.3.1 OVERSAMPLING"""

f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))

colors = ['#B3F9C5', '#f9c5b3']
# Boxplots với dữ liệu không dữ liệu ngoại vi đã được lượt bỏ
# Feature errorBalanceOrig
sns.boxplot(x="isFraud", y="errorBalanceOrig", data=df_with_overspampling,ax=ax1, palette=colors)
ax1.set_title("errorBalanceOrig Feature \n Reduction of outliers", fontsize=14)

# # Feature errorBalanceDest
sns.boxplot(x="isFraud", y="errorBalanceDest", data=df_with_overspampling, ax=ax2, palette=colors)
ax2.set_title("errorBalanceDest Feature \n Reduction of outliers", fontsize=14)

# # Feature oldbalanceDest
sns.boxplot(x="isFraud", y="oldbalanceDest", data=df_with_overspampling, ax=ax3, palette=colors)
ax3.set_title("oldbalanceDest Feature \n Reduction of outliers", fontsize=14)

plt.show()

"""###6.6 Lưu dữ liệu sau khi thực hiện cân bằng dữ liệu"""

from google.colab import drive
drive.mount('drive')

"""####6.6.1 UNDERSAMPLING"""

# Dự liệu sau khi làm sạch bằng undersamping, dùng để train.
new_df.to_csv('/content/drive/My Drive/Colab Notebooks/Graduation Thesis/Data/data_under_sampling.csv')
print('=====DONE=====')

"""####6.6.2 OVERSAMPLING"""

# Dự liệu sau khi làm sạch bằng oversamping, dùng để train.
df_with_overspampling.to_csv('/content/drive/My Drive/Colab Notebooks/Graduation Thesis/Data/data_over_sampling.csv')
print('=====DONE=====')

"""##7.Mô hình phát hiện gian lận với dữ liệu mất cân bằng"""

# Bỏ qua nếu lấy dữ liệu trực tiếp (không cần qua quá trình chỉnh làm sạch)
data_train = X

data_train.head()

df_with_overspampling['isFraud'].count_values

data_under_sampling.head()

len(data_under_sampling)

print('Sô lượng giao dịch với overspling: ', len(df_with_overspampling))
df_with_overspampling.head()

"""###Các thuật toán sẽ tiếp cận

**Các thuật toán Lựa chọn để huấn luyện:**
- Random Forest
- ***Extreme Gradient Boosting (EGB).***

####Tách dữ liệu thành tập huấn luyện và tập kiểm thử
- Tách theo tỷ lệ 80:20 (train - test)
"""

# Tách các thuộc tính độc lập và thuộc tính phụ thuộc
y = data_train['isFraud']
X = data_train.drop('isFraud', axis=1)

# Chia tập dữ liệu 2 phân: tập huấn luyện và tập kiểm thử 
# Tỷ lệ: train 80%, test 20%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyển các giá trị thành 1 mảng để đưa vào các thuật toán
# thuật toán XGBoost không cần thực hiện chuyển đổi này
X_train = X_train.values
X_test = X_test.values
y_train = y_train.values
y_test = y_test.values

# Hàm vẽ confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=14)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

"""###7.1 Thuật toán Rừng Ngẫu Nhiên Random Forest

#####7.1.1 Với dữ liệu chưa xử lý cân bằng
"""

# Load data
# data = pd.read_csv('data_name.csv', index_col=0)
data_clean = data_clean.iloc[:, 1:]
X = data_clean.drop('isFraud', axis=1)
y = data_clean['isFraud']

# Tách dữ liệu thành train set và test set
# Chúng tôi sẽ sử dụng xác thực chéo trên tập huấn luyện để điều chỉnh các tham số, sau đó kiểm tra dữ liệu chưa xem
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42)

print(y.value_counts())
print(y_train.value_counts())
print(y_test.value_counts())

# Mô hình
model = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', random_state=42)
# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []
# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_models = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []
# KFold = 3
kf = StratifiedKFold(n_splits=3)
kf.get_n_splits(X_train, y_train)
# Thời gian bắt đầu train
start_ho = timer()
for train_index, test_index in kf.split(X_train, y_train):
    X_tr, X_te = X_train.iloc[train_index], X_train.iloc[test_index]
    y_tr, y_te = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_tr, y_tr)
    y_pre_train = model.predict(X_te)
    y_pre_test = model.predict(X_test)
    # Print the accuracy
    accuracy_models.append(
        accuracy_score(y_te, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)
    # Confusion matrix
    conf_matrix = confusion_matrix(y_te, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)
    # F1 score
    f1_score_train.append(f1_score(y_te, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))
# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_models)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)
print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_models))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))
print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))
# Thời gian kết thúc train
end_ho = timer()
# Tổng thời thực hiện
time_ho = (end_ho - start_ho)
print('Thời gian thực hiện: ', time_ho, ' giây')

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""#####7.1.2 Với dữ liệu đã xử lý mất cân bằng - overfiting"""

print(df_with_overspampling['isFraud'].value_counts())
df_with_overspampling.head()

# df_with_overspampling = df_with_overspampling.iloc[:, 1:]
# Tách các thuộc tính độc lập và thuộc tính phụ thuộc
y = df_with_overspampling['isFraud']
X = df_with_overspampling.drop('isFraud', axis=1)

# Chia tập dữ liệu 2 phân: tập huấn luyện và tập kiểm thử 
# Tỷ lệ: train 80%, test 20%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(y.value_counts())
print(y_train.value_counts())
print(y_test.value_counts())

# Mô hình
model = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', random_state=42)
# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []
# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_models = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []
# KFold = 3
kf = KFold(n_splits=3)
kf.get_n_splits(X_train)
# Thời gian bắt đầu train
start_ho = timer()
for train_index, test_index in kf.split(X_train):
    X_tr, X_te = X_train.iloc[train_index], X_train.iloc[test_index]
    y_tr, y_te = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_tr, y_tr)
    # Print the accuracy
    y_pre_train = model.predict(X_te)
    y_pre_test = model.predict(X_test)
    accuracy_models.append(
        accuracy_score(y_te, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)
    # Confusion matrix
    conf_matrix = confusion_matrix(y_te, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)
    # F1 score
    f1_score_train.append(f1_score(y_te, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))
# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_models)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)
print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_models))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))
print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))
# Thời gian kết thúc train
end_ho = timer()
# Tổng thời thực hiện
time_ho = (end_ho - start_ho)
print('Thời gian thực hiện: ', time_ho, ' giây')

actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Fraud', 'Not Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""#####7.1.3 Với dữ liệu đã xử lý mất cân bằng - underfiting"""

print(data_under_sampling['isFraud'].value_counts())
data_under_sampling.head()

# data_under_sampling = data_under_sampling.iloc[:, 1:]
# Tách các thuộc tính độc lập và thuộc tính phụ thuộc
y = data_under_sampling['isFraud']
X = data_under_sampling.drop('isFraud', axis=1)

# Chia tập dữ liệu 2 phân: tập huấn luyện và tập kiểm thử 
# Tỷ lệ: train 80%, test 20%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(y.value_counts())
print(y_train.value_counts())
print(y_test.value_counts())

# Mô hình
model = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', random_state=42)
# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []
# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_models = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []
# KFold = 3
kf = KFold(n_splits=3)
kf.get_n_splits(X_train)
# Thời gian bắt đầu train
start_ho = timer()
for train_index, test_index in kf.split(X_train):
    X_tr, X_te = X_train.iloc[train_index], X_train.iloc[test_index]
    y_tr, y_te = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_tr, y_tr)
    # Print the accuracy
    y_pre_train = model.predict(X_te)
    y_pre_test = model.predict(X_test)
    accuracy_models.append(
        accuracy_score(y_te, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)
    # Confusion matrix
    conf_matrix = confusion_matrix(y_te, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)
    # F1 score
    f1_score_train.append(f1_score(y_te, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))
# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_models)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)
print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_models))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))
print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))
# Thời gian kết thúc train
end_ho = timer()
# Tổng thời thực hiện
time_ho = (end_ho - start_ho)
print('Thời gian thực hiện: ', time_ho, ' giây')

actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Fraud', 'Not Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###7.2 Thuật toán xgboost(Extreme Gradient Boosting)

#####7.2.1 Với dữ liệu chưa xử lý cân bằng
"""

data_clean['isFraud'].value_counts()

# Load data
data_clean = data_clean.iloc[:, 1:]
X = data_clean.drop('isFraud', axis=1)
y = data_clean['isFraud']

# Tách dữ liệu thành train set và test set
# Chúng tôi sẽ sử dụng xác thực chéo trên tập huấn luyện để điều chỉnh các tham số, sau đó kiểm tra dữ liệu chưa xem
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42)

print(y.value_counts())
print(y_train.value_counts())
print(y_test.value_counts())

# Mô hình
model = XGBClassifier(objective="binary:logistic", random_state=42)
# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []
# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_models = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []
# KFold = 3
kf = StratifiedKFold(n_splits=3)
kf.get_n_splits(X_train, y_train)
# Thời gian bắt đầu train
start_ho = timer()
for train_index, test_index in kf.split(X_train, y_train):
    X_tr, X_te = X_train.iloc[train_index], X_train.iloc[test_index]
    y_tr, y_te = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_tr, y_tr)
    y_pre_train = model.predict(X_te)
    y_pre_test = model.predict(X_test)
    # Print the accuracy
    accuracy_models.append(
        accuracy_score(y_te, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)
    # Confusion matrix
    conf_matrix = confusion_matrix(y_te, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)
    # F1 score
    f1_score_train.append(f1_score(y_te, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))
# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_models)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)
print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_models))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))
print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))
# Thời gian kết thúc train
end_ho = timer()
# Tổng thời thực hiện
time_ho = (end_ho - start_ho)
print('Thời gian thực hiện: ', time_ho, ' giây')

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""**Nhận xét:**
- Có 828658 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 0 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 12 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 2452 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

#####7.2.2 Với dữ liệu đã xử lý cân bằng - Overfiting
"""

print(df_with_overspampling['isFraud'].value_counts())
df_with_overspampling.head()

# Load data
df_with_overspampling = df_with_overspampling.iloc[:, 1:]
X = df_with_overspampling.drop('isFraud', axis=1)
y = df_with_overspampling['isFraud']

# Tách dữ liệu thành train set và test set
# Chúng tôi sẽ sử dụng xác thực chéo trên tập huấn luyện để điều chỉnh các tham số, sau đó kiểm tra dữ liệu chưa xem
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42)

print(y.value_counts())
print(y_train.value_counts())
print(y_test.value_counts())

# Mô hình
model = XGBClassifier(objective="binary:logistic", random_state=42)
# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []
# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_models = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []
# KFold = 3
kf = StratifiedKFold(n_splits=3)
kf.get_n_splits(X_train, y_train)
# Thời gian bắt đầu train
start_ho = timer()
for train_index, test_index in kf.split(X_train, y_train):
    X_tr, X_te = X_train.iloc[train_index], X_train.iloc[test_index]
    y_tr, y_te = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_tr, y_tr)
    y_pre_train = model.predict(X_te)
    y_pre_test = model.predict(X_test)
    # Print the accuracy
    accuracy_models.append(
        accuracy_score(y_te, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)
    # Confusion matrix
    conf_matrix = confusion_matrix(y_te, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)
    # F1 score
    f1_score_train.append(f1_score(y_te, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))
# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_models)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)
print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_models))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))
print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))
# Thời gian kết thúc train
end_ho = timer()
# Tổng thời thực hiện
time_ho = (end_ho - start_ho)
print('Thời gian thực hiện: ', time_ho, ' giây')

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Fraud', 'Not Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""**Nhận xét:**
- Có 828658 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 0 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 12 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 2452 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

#==============================================

#Đoạn này dùng để ra hội đồng - câu 13
"""

import pickle
# Save to file in the current working directory
xgboost_oversampling = "xgboost_oversampling.pkl"
with open(xgboost_oversampling, 'wb') as file:
    pickle.dump(model, file)

# Load from file
with open(xgboost_oversampling, 'rb') as file:
    pickle_model = pickle.load(file)

X_test_test =  X_test.head()
X_test_test

Ypredict = pickle_model.predict(X_test_test)
Ypredict