# -*- coding: utf-8 -*-
"""Khóa luận 2 - Credit Fraud -  15520214 - 15520452.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T9pQG08NDlbv03P7-GBeqpljH5PinabT

Sinh viên thực hiện:
 - Nguyễn Hoàng Hiệp - 15520214
 - Nguyễn Hoàng Luân -

##1.Khai báo các thư viện cần thiết
"""

# Imported Libraries
#Thư viện xử lý số
import numpy as np
import pandas as pd
from pandas import read_csv
import matplotlib.pyplot as plt
import seaborn as sns
from time import time
from timeit import default_timer as timer
import itertools
from scipy.stats import norm

# # Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections
from sklearn.preprocessing import StandardScaler, RobustScaler
from xgboost.sklearn import XGBClassifier
from imblearn.combine import SMOTEENN

# # Other Libraries
from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold, train_test_split
from sklearn.model_selection import train_test_split, RandomizedSearchCV

import warnings
warnings.filterwarnings("ignore")

# Hàm vẽ confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=14)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

"""##2.Kết nối vơi google driver và lấy dữ liệu
Cách này có thể chia sẻ cho mọi người có thể  thể chạy thử nghiệm
"""

# Import PyDrive and associated libraries.
# This only needs to be done once per notebook.
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Download a file based on its file ID.
#  là id đường dẫn chia sẽ file tài liều
# example: https://drive.google.com/file/d/1rNwi9Ki1ujzHH4Q0rzXImW47tbZmR3Ds/view
# ID : 1rNwi9Ki1ujzHH4Q0rzXImW47tbZmR3Ds
file_id = '1rNwi9Ki1ujzHH4Q0rzXImW47tbZmR3Ds'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('creditcard.csv') # dowload file
# Đọc dữ liệu
data_raw = pd.read_csv('creditcard.csv')

# Link: https://drive.google.com/file/d/1ustrq4LwCMewdoDxf9SI89RaCxjn0ZLR/view?usp=sharing
file_id = '1ustrq4LwCMewdoDxf9SI89RaCxjn0ZLR'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('data_credit_undersampling.csv')
# Đọc dữ liệu
data_undersampling = pd.read_csv('data_credit_undersampling.csv')



"""##3.Hiểu dữ liệu - Understanding data"""

data_raw.head()

data_raw.describe()

# Đếm số lượng giao dịch có dữ liệu null
data_raw.isnull().sum().max()

data_raw.columns

# Số lượng trong các lớp cần phân loại
print('No Frauds', round(data_raw['Class'].value_counts()[0]/len(data_raw) * 100,2), '% of the dataset')
print('Frauds', round(data_raw['Class'].value_counts()[1]/len(data_raw) * 100,2), '% of the dataset')

"""Lưu ý: Cách mất cân bằng là tập dữ liệu gốc của chúng tôi!  Hầu hết các giao dịch là không gian lận.  Nếu chúng tôi sử dụng khung dữ liệu này làm cơ sở cho các mô hình và phân tích dự đoán của mình, chúng tôi có thể gặp rất nhiều lỗi và thuật toán của chúng tôi có thể sẽ phù hợp vì nó sẽ "cho rằng" hầu hết các giao dịch không phải là gian lận.  Nhưng chúng tôi không muốn mô hình của mình giả định, chúng tôi muốn mô hình của chúng tôi phát hiện các mô hình có dấu hiệu lừa đảo!

**Tóm lược:**

- Số tiền giao dịch tương đối nhỏ.  Giá trị trung bình của tất cả các gắn kết được thực hiện là khoảng 88 USD.
- Không có giá trị "Null", vì vậy chúng tôi không phải tìm cách thay thế giá trị.
- Hầu hết các giao dịch là Không gian lận (99,83%) trong khi giao dịch Gian lận xảy ra (017%) thời gian trong khung dữ liệu.

**Feature Technicalities:**  
- PCA Transformation: Mô tả dữ liệu nói rằng tất cả các tính năng đã trải qua chuyển đổi PCA (Kỹ thuật giảm kích thước) (Ngoại trừ thời gian và số tiền giao dịch).
- Scaling: Để thực hiện tính năng chuyển đổi PCA cần phải được thu nhỏ trước đó. (Trong trường hợp này, tất cả các tính năng V đã được thu nhỏ hoặc ít nhất đó là những gì chúng tôi giả định những người phát triển bộ dữ liệu đã làm.)
"""

colors = ["#0101DF", "#DF0101"]

sns.countplot('Class', data=data_raw, palette=colors)
plt.title('Phân Phối Lớp Phân Loại \n (0: Không gian lận || 1: Gian lận)', fontsize=14)

"""Distributions (Phân phối):
- Bằng cách xem các bản phân phối, chúng ta có thể biết các tính năng này bị lệch như thế nào, chúng ta cũng có thể thấy các bản phân phối tiếp theo của các tính năng khác.  Có những kỹ thuật có thể giúp các bản phân phối ít bị sai lệch sẽ được thực hiện trong ở trong các phần sau.
"""

fig, ax = plt.subplots(1, 2, figsize=(18,4))

amount_val = data_raw['Amount'].values
time_val = data_raw['Time'].values

sns.distplot(amount_val, ax=ax[0], color='r')
ax[0].set_title('Phân phối của thuộc tính Số tiền (Amount)', fontsize=14)
ax[0].set_xlim([min(amount_val), max(amount_val)])

sns.distplot(time_val, ax=ax[1], color='b')
ax[1].set_title('Phân phối của thuộc tính Thời Gian (Time)', fontsize=14)
ax[1].set_xlim([min(time_val), max(time_val)])

plt.show()

"""#4.Scaling and Distributing (Chia tỷ lệ và phân phối)

- Chúng tôi sẽ chia tỷ lệ các cột bao gồm `Thời gian` và Số tiền. Thời gian và số lượng nên được thu nhỏ như các cột khác.
- Chúng tôi cũng cần tạo một mẫu phụ(sub-sample) của bộ dữ liệu để có số lượng các trường hợp Gian lận và Không gian lận tương đương, giúp thuật toán của chúng tôi hiểu rõ hơn các mẫu xác định liệu giao dịch có lừa đảo hay không.

- Mẫu phụ (Sub-sample) là chúng tôi sẽ có bộ dữ liệu với tỷ 50/50 của các giao dịch gian lận và không gian lận.

- Chúng ta cần tạo sub-sample vì:
    - Overfitting: Các mô hình phân loại của chúng tôi sẽ cho rằng trong hầu hết các trường hợp không có gian lận! Những gì chúng tôi muốn cho mô hình của chúng tôi là phát hiện được gian lận.
    - Wrong Correlations: Mặc dù chúng tôi không biết các tính năng "V" là gì, nhưng sẽ hữu ích khi hiểu từng tính năng này ảnh hưởng đến kết quả như thế nào (Gian lận hoặc Không gian lận) bằng cách có một khung dữ liệu mất cân bằng mà chúng tôi không thể thấy được mối tương quan thực sự giữa các lớp và tính năng.

**Tóm lượt:**
- Có 492 trường hợp gian lận trong bộ dữ liệu của chúng tôi để chúng tôi có thể lấy ngẫu nhiên 492 trường hợp không gian lận để tạo khung dữ liệu phụ mới của chúng tôi.
- Chúng tôi kết hợp 492 trường hợp gian lận và không gian lận, tạo ra một mẫu phụ mới.
"""

# ***
std_scaler = StandardScaler()
rob_scaler = RobustScaler()
# 
data_raw['scaled_amount'] = rob_scaler.fit_transform(data_raw['Amount'].values.reshape(-1,1))
data_raw['scaled_time'] = rob_scaler.fit_transform(data_raw['Time'].values.reshape(-1,1))

data_raw.drop(['Time','Amount'], axis=1, inplace=True)

scaled_amount = data_raw['scaled_amount']
scaled_time = data_raw['scaled_time']

data_raw.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)
data_raw.insert(0, 'scaled_amount', scaled_amount)
data_raw.insert(1, 'scaled_time', scaled_time)

data_raw.head()

"""#5.Tách dữ liệu (Splitting the Data)

Trước khi tiến hành kỹ thuật Lấy mẫu ngẫu nhiên, chúng tôi phải tách khung dữ liệu ban đầu.  Với mục đích thử nghiệm, hãy nhớ rằng mặc dù chúng tôi đang phân tách dữ liệu khi triển khai các kỹ thuật Lấy mẫu ngẫu nhiên(Random UnderSampling) hoặc Lấy mẫu quá mức(Random Oversampling). Mục tiêu là để để các mô hình của chúng tôi phát hiện các giao dịch gian lận và kiểm tra nó trên bộ thử nghiệm ban đầu (Dữ liệu mà nó chưa từng thấy).
"""

print('No Frauds', round(data_raw['Class'].value_counts()[0]/len(data_raw) * 100,2), '% of the dataset')
print('Frauds', round(data_raw['Class'].value_counts()[1]/len(data_raw) * 100,2), '% of the dataset')

X = data_raw.drop('Class', axis=1)
y = data_raw['Class']

original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(
    X, y, test_size=0.33, random_state=42, stratify=y)

# Check the Distribution of the labels
# Chuyển đổi thành một mảng
original_Xtrain = original_Xtrain.values
original_Xtest = original_Xtest.values
original_ytrain = original_ytrain.values
original_ytest = original_ytest.values

#
train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)
test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)
print('-' * 80)

print('Phân phối nhãn phân loại:')
print(train_counts_label / len(original_ytrain))
print(test_counts_label / len(original_ytest))

"""#6.Kỹ thuật lấy mẫu ngẫu nhiên dưới mức (Random Under-Sampling)

- Chúng tôi sẽ thực hiện kỹ thuật "Lấy mẫu ngẫu nhiên", về cơ bản bao gồm xóa dữ liệu để có bộ dữ liệu cân bằng hơn và do đó tránh các mô hình của chúng tôi bị quá mức.

Các bước thực hiện:**bold text**
- Điều đầu tiên chúng ta phải làm là xác định mức độ mất cân bằng của lớp chúng ta (sử dụng "value_counts ()" trên cột lớp để xác định số lượng cho mỗi nhãn)
- Khi chúng tôi xác định có bao nhiêu trường hợp được coi là giao dịch gian lận (Gian lận = "1"), chúng tôi nên xóa các giao dịch không gian lận cho đến khi cùng số lượng với các giao dịch gian lận (giả sử chúng tôi muốn tỷ lệ 50/50), điều này sẽ tương đương với 492  các trường hợp gian lận và 492 trường hợp giao dịch không gian lận.
- Sau khi thực hiện kỹ thuật này, chúng tôi có một mẫu phụ của khung dữ liệu của chúng tôi với tỷ lệ 50/50 liên quan đến các lớp của chúng tôi.  Sau đó, bước tiếp theo chúng tôi sẽ thực hiện là xáo trộn dữ liệu để xem liệu các mô hình của chúng tôi có thể duy trì độ chính xác nhất định mỗi khi chúng tôi chạy lại tập lệnh này hay không.

**Lưu ý**: Vấn đề chính với "Lấy mẫu ngẫu nhiên dưới mức" là chúng tôi gặp rủi ro là các mô hình phân loại của chúng tôi sẽ không hoạt động chính xác như chúng tôi mong muốn vì có rất nhiều thông tin bị mất mát  (Chỉ còn 492 giao dịch không gian lận từ 284.315  giao dịch không gian lận), các dữ liệu bị xóa đi có thể chứ nhiều thông tin có ích.
"""

# Xáo trộn dữ liệu
data_raw = data_raw.sample(frac=1)

# Số lượng giao dịch gian lận là 492 dòng.
fraud_df = data_raw.loc[data_raw['Class'] == 1]
non_fraud_df = data_raw.loc[data_raw['Class'] == 0][:492]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

# Xáo trộn dữ liệu một lần nữa
new_df = normal_distributed_df.sample(frac=1, random_state=42)

new_df.head()

print('Phân phối của các nhãn phân loại trong tệp dữ liệu mới')
print(new_df['Class'].value_counts()/len(new_df))
colors = ["#0101DF", "#DF0101"]
sns.countplot('Class', data=new_df, palette=colors)
plt.title('Equally Distributed Classes', fontsize=14)
plt.show()

"""##6.1 Ma trận tương quan (Correlation Matrices)

- Ma trận tương quan là bản chất của việc hiểu dữ liệu của chúng tôi.  Chúng tôi muốn biết liệu có những tính năng ảnh hưởng lớn đến việc một giao dịch cụ thể có phải là lừa đảo hay không. Tuy nhiên, điều quan trọng là chúng tôi sử dụng đúng tệp dữ liệu (undersampling) để chúng tôi xem các tính năng nào có mối tương quan thuận hoặc ngược cao liên quan đến các giao dịch gian lận.
"""

f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))

# Toàn bộ dữ liệu
corr = data_raw.corr()
sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)
ax1.set_title("Ma trận tương quan dữ liệu mất cân bằng", fontsize=14)

# Dữ bộ dữ liệu undersampling
sub_sample_corr = new_df.corr()
sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)
ax2.set_title('Ma trận tương quan dữ liệu cân bằng', fontsize=14)
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))

sns.boxplot(x="Class", y="V17", data=new_df, palette=colors, ax=axes[0])
axes[0].set_title('V17 vs Class Tương quan nghịch')

sns.boxplot(x="Class", y="V14", data=new_df, palette=colors, ax=axes[1])
axes[1].set_title('V14 vs Class Tương quan nghịch')


sns.boxplot(x="Class", y="V12", data=new_df, palette=colors, ax=axes[2])
axes[2].set_title('V12 vs Class Tương quan nghịch')


sns.boxplot(x="Class", y="V10", data=new_df, palette=colors, ax=axes[3])
axes[3].set_title('V10 vs Class Tương quan nghịch')

plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))

sns.boxplot(x="Class", y="V11", data=new_df, palette=colors, ax=axes[0])
axes[0].set_title('V11 vs Class Tương quan thuận')

sns.boxplot(x="Class", y="V4", data=new_df, palette=colors, ax=axes[1])
axes[1].set_title('V4 vs Class Tương quan thuận')

sns.boxplot(x="Class", y="V2", data=new_df, palette=colors, ax=axes[2])
axes[2].set_title('V2 vs Class Tương quan thuận')

sns.boxplot(x="Class", y="V19", data=new_df, palette=colors, ax=axes[3])
axes[3].set_title('V19 vs Class Tương quan thuận')

plt.show()

"""**Tóm tắt và giải thích**:
- Tương quan ngược: các thuộc tính V17, V14, V12 và V10 có mối tương quan ngược chiều. Lưu ý rằng các giá trị này càng thấp thì kết quả cuối cùng sẽ là một giao dịch gian lận.
- Tương quan tích cực: V2, V4, V11 và V19 có mối tương quan tích cực.  Lưu ý rằng các giá trị này càng cao thì kết quả cuối cùng sẽ là một giao dịch gian lận.
- Biểu đồ hộp (BoxPlots): Chúng tôi sẽ sử dụng biểu đồ hộp để hiểu rõ hơn về việc phân phối các tính năng này trong các giao dịch gian lận và không gian lận.

##6.2 Phát hiện bất thường (Anomaly Detection)

- Mục đích của chúng tôi là loại bỏ "các ngoại lệ cực đoan" khỏi các tính năng có mối tương quan cao với các nhãn của chúng tôi. Điều này sẽ có tác động tích cực đến độ chính xác của các mô hình của chúng tôi.

**Phương pháp phạm vi liên vùng (Interquartile Range Method):**
- Phạm vi liên vùng (IQR): Chúng tôi tính toán điều này bằng sự khác biệt giữa phân vị thứ 75 và phân vị thứ 25. Mục đích của chúng tôi là tạo ra một ngưỡng vượt quá tỷ lệ phần trăm thứ 75 và 25 mà trong trường hợp một số trường hợp vượt qua ngưỡng này, có thể sẽ bị xóa.
- Biểu đồ hộp (Boxplots): Bên cạnh việc dễ dàng nhìn thấy các phần trăm thứ 25 và 75 (cả hai đầu của hình vuông), chúng ta cũng dễ dàng nhìn thấy các ngoại lệ cực đoan (các điểm nằm ngoài cực thấp và cực cao hơn).

**Lưu ý:**
Chúng ta phải cẩn thận về ngưỡng để loại bỏ các ngoại lệ. Chúng tôi xác định ngưỡng bằng cách nhân một số (ví dụ: 1,5) với (Phạm vi liên dải).  Ngưỡng này càng cao, càng ít ngoại lệ sẽ phát hiện và ngưỡng này càng thấp thì càng phát hiện ra nhiều ngoại lệ.
"""

f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))

v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values
sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')
ax1.set_title('Phân phối thuộc tính V14 \n (Giao dịch gian lận)', fontsize=14)

v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values
sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')
ax2.set_title('Phân phối thuộc tính V12 \n (Giao dịch gian lận)', fontsize=14)


v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values
sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')
ax3.set_title('Phân phối thuộc tính V10 \n (Giao dịch gian lận)', fontsize=14)

plt.show()

# # -----> V14 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values
q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)
print('Bách phân bị 25: {} | Bách phân vị 75: {}'.format(q25, q75))
v14_iqr = q75 - q25
print('iqr: {}'.format(v14_iqr))

v14_cut_off = v14_iqr * 1.5
v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off
print('V14 Giá trị biên dưới: {}'.format(v14_lower))
print('V14 Giá trị biên trên: {}'.format(v14_upper))

outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]
print('Tính năng V14 có số giá trị ngoại vi là: {}'.format(len(outliers)))

new_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(new_df)))
print('----' * 44)

# -----> V12 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values
q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)
v12_iqr = q75 - q25

v12_cut_off = v12_iqr * 1.5
v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off
print('V12 Giá trị biên dưới: {}'.format(v12_lower))
print('V12 Giá trị biên trên: {}'.format(v12_upper))
outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]
print('Tính năng V12 có số giá trị ngoại vi là: {}'.format(len(outliers)))

new_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(new_df)))
print('----' * 44)


# V10 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values
q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)
v10_iqr = q75 - q25

v10_cut_off = v10_iqr * 1.5
v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off
print('V10 Giá trị biên dưới: {}'.format(v10_lower))
print('V10 Giá trị biên trên: {}'.format(v10_upper))
outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]
print('Tính năng V10 có số giá trị ngoại vi là: {}'.format(len(outliers)))
new_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(new_df)))

f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))

colors = ['#B3F9C5', '#f9c5b3']
# Boxplots sau khi bỏ các giá trị ngoại vi
# Thuộc tính V14
sns.boxplot(x="Class", y="V14", data=new_df,ax=ax1, palette=colors)
ax1.set_title("V14 Feature \n Reduction of outliers", fontsize=14)
ax1.annotate('Fewer extreme \n outliers', xy=(0.98, -17.5), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)

# Thuộc tính 12
sns.boxplot(x="Class", y="V12", data=new_df, ax=ax2, palette=colors)
ax2.set_title("V12 Feature \n Reduction of outliers", fontsize=14)
ax2.annotate('Fewer extreme \n outliers', xy=(0.98, -17.3), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)

# Thuộc tính V10
sns.boxplot(x="Class", y="V10", data=new_df, ax=ax3, palette=colors)
ax3.set_title("V10 Feature \n Reduction of outliers", fontsize=14)
ax3.annotate('Fewer extreme \n outliers', xy=(0.95, -16.5), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)


plt.show()

"""**Tóm lược:**
- Trực quan hóa phân phối: Trước tiên chúng tôi bắt đầu bằng cách hình dung phân phối tính năng mà chúng tôi sẽ sử dụng để loại bỏ một số ngoại lệ.  V14 là tính năng duy nhất có phân phối Gaussian so với các tính năng V12 và V10.
- Xác định ngưỡng: Sau khi chúng tôi quyết định số nào, chúng tôi sẽ sử dụng để nhân với iqr (loại bỏ nhiều ngoại lệ hơn), chúng tôi sẽ tiến hành xác định ngưỡng trên và dưới bằng cách đặt ngưỡng q25 (ngưỡng cực thấp) và thêm ngưỡng q75 +  (ngưỡng cực cao).

##6.3Lưu tệp dữ liệu đã random underSampling
"""

from google.colab import drive
drive.mount('drive')

# Dự liệu sau khi làm sạch bằng undersamping, dùng để train.
new_df.to_csv('/content/drive/My Drive/Colab Notebooks/Graduation Thesis/Data/data_credit_undersampling.csv')
print('=====DONE=====')

"""##6.4 Thuật toán Random forest và UnderSampling"""

X = new_df.drop('Class', axis=1)
y = new_df['Class']

# X = data_undersampling.drop('Class', axis=1)
# y = data_undersampling['Class']

print('Số  lượng giao dịch trên từng nhãn:\n', y.value_counts())

# Tách dữ liệu thành train set và test set
# Chúng tôi sẽ sử dụng xác thực chéo trên tập huấn luyện để điều chỉnh các tham số, sau đó kiểm tra dữ liệu chưa xem
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42)

print('Số  lượng giao dịch trên từng nhãn trên dữ liệu huấn luyện:\n')
print(y_train.value_counts())
print('Số  lượng giao dịch trên từng nhãn trên dữ liệu kiểm thử:\n')
print(y_test.value_counts())

# Mô hình
model = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    undersample_Xtrain, undersample_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    undersample_ytrain, undersample_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(undersample_Xtrain, undersample_ytrain)
    y_pre_train = model.predict(undersample_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(undersample_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(undersample_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(undersample_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 144 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 3 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 16 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 121 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

##6.5 Thuật toán XGBoost và kỹ thuật UnderSampling
"""

# Mô hình
model = XGBClassifier(objective="binary:logistic", random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    undersample_Xtrain, undersample_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    undersample_ytrain, undersample_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(undersample_Xtrain, undersample_ytrain)
    y_pre_train = model.predict(undersample_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(undersample_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(undersample_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(undersample_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 141 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 6 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 14 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 123 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

#7.Kỹ thuật SMOTE (Over-Sampling):

SMOTE là viết tắt cuả Synthetic Minority Over-sampling Technique
- Không giống như Random UnderSampling, SMOTE tạo các điểm tổng hợp mới để có sự cân bằng của các lớp. Đây là một cách khác để giải quyết "các vấn đề mất cân bằng".

**Hiểu về SMOTE:**
- SMOTE tạo ra các dữ liệu tổng hợp(synthetic points) từ lớp thiểu số để đạt được sự cân bằng giữa nhóm thiểu số và đa số.
- Vị trí của các synthetic points: SMOTE chọn khoảng cách giữa các lân cận gần nhất của nhóm thiểu số, ở giữa các khoảng cách này, nó tạo ra các điểm tổng hợp.
- Nhiều thông tin được giữ lại vì chúng tôi không phải xóa bất kỳ hàng nào không giống như trong việc lấy mẫu ngẫu nhiên.
- Mặc dù có khả năng SMOTE sẽ chính xác hơn so với lấy mẫu ngẫu nhiên, nhưng sẽ mất nhiều thời gian hơn để đào tạo vì không có hàng nào bị loại bỏ như đã nêu trước đây.
"""

X = data_raw.drop('Class', axis=1)
y = data_raw['Class']

print("Trước SMOTE, số lượng giao dịch gian lận là: {}".format(sum(y==1)))
print("Trước SMOTE, số lượng giao dịch không gian lận là: {} \n".format(sum(y==0)))

sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_sample(X, y.ravel())

print('Sau SMOTE, Kích thước của dữ liệu train_X: {}'.format(X_res.shape))
print('Sau SMOTE, Kích thước của dữ liệu train_y: {} \n'.format(y_res.shape))

print("Sau SMOTE, Số lượng giao dịch gian lận là: {}".format(sum(y_res==1)))
print("Sau SMOTE, Số lượng giao dịch không gian lận là: {}".format(sum(y_res==0)))

# Convert 'ndarray' to 'DataFrame'
X_res = pd.DataFrame(X_res, columns=X.columns)
y_res = pd.DataFrame(y_res, columns=['Class'])
print(X_train.shape)
print(y_train.shape)

data_smote = pd.concat([X_res, y_res], axis=1)

print(data_smote.shape)

"""##7.1 Ma trận tương quan"""

plt.figure(figsize=(22,10))
# Dữ bộ dữ liệu OverSampling
smote_corr = data_smote.corr()
ax = sns.heatmap(
    smote_corr, cmap='coolwarm_r', annot_kws={'size':20},
    xticklabels=corr.columns, yticklabels=corr.columns)
plt.show()

"""##7.2 Phát hiện bất thường (Anomaly Detection)"""

f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))

v14_fraud_dist = data_smote['V14'].loc[data_smote['Class'] == 1].values
sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')
ax1.set_title('Phân phối thuộc tính V14 \n (Giao dịch gian lận)', fontsize=14)

v12_fraud_dist = data_smote['V12'].loc[data_smote['Class'] == 1].values
sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')
ax2.set_title('Phân phối thuộc tính V12 \n (Giao dịch gian lận)', fontsize=14)


v10_fraud_dist = data_smote['V10'].loc[data_smote['Class'] == 1].values
sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')
ax3.set_title('Phân phối thuộc tính V10 \n (Giao dịch gian lận)', fontsize=14)

plt.show()

v14_fraud_dist# # -----> V14 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v14_fraud = data_smote['V14'].loc[data_smote['Class'] == 1].values
q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)
print('Bách phân bị 25: {} | Bách phân vị 75: {}'.format(q25, q75))
v14_iqr = q75 - q25
print('iqr: {}'.format(v14_iqr))

v14_cut_off = v14_iqr * 1.5
v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off
print('V14 Giá trị biên dưới: {}'.format(v14_lower))
print('V14 Giá trị biên trên: {}'.format(v14_upper))

outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]
print('Tính năng V14 có số giá trị ngoại vi là: {}'.format(len(outliers)))

data_smote = data_smote.drop(data_smote[(data_smote['V14'] > v14_upper) | (data_smote['V14'] < v14_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(data_smote)))
print('----' * 44)

# -----> V12 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v12_fraud = data_smote['V12'].loc[data_smote['Class'] == 1].values
q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)
v12_iqr = q75 - q25

v12_cut_off = v12_iqr * 1.5
v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off
print('V12 Giá trị biên dưới: {}'.format(v12_lower))
print('V12 Giá trị biên trên: {}'.format(v12_upper))
outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]
print('Tính năng V12 có số giá trị ngoại vi là: {}'.format(len(outliers)))

data_smote = data_smote.drop(data_smote[(data_smote['V12'] > v12_upper) | (data_smote['V12'] < v12_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(data_smote)))
print('----' * 44)


# V10 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v10_fraud = data_smote['V10'].loc[data_smote['Class'] == 1].values
q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)
v10_iqr = q75 - q25

v10_cut_off = v10_iqr * 1.5
v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off
print('V10 Giá trị biên dưới: {}'.format(v10_lower))
print('V10 Giá trị biên trên: {}'.format(v10_upper))
outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]
print('Tính năng V10 có số giá trị ngoại vi là: {}'.format(len(outliers)))
data_smote = data_smote.drop(data_smote[(data_smote['V10'] > v10_upper) | (data_smote['V10'] < v10_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(data_smote)))

f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))

colors = ['#B3F9C5', '#f9c5b3']
# Boxplots sau khi bỏ các giá trị ngoại vi
# Thuộc tính V14
sns.boxplot(x="Class", y="V14", data=data_smote,ax=ax1, palette=colors)
ax1.set_title("V14 Feature \n Reduction of outliers", fontsize=14)
ax1.annotate('Fewer extreme \n outliers', xy=(0.98, -17.5), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)

# Thuộc tính 12
sns.boxplot(x="Class", y="V12", data=data_smote, ax=ax2, palette=colors)
ax2.set_title("V12 Feature \n Reduction of outliers", fontsize=14)
ax2.annotate('Fewer extreme \n outliers', xy=(0.98, -17.3), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)

# Thuộc tính V10
sns.boxplot(x="Class", y="V10", data=data_smote, ax=ax3, palette=colors)
ax3.set_title("V10 Feature \n Reduction of outliers", fontsize=14)
ax3.annotate('Fewer extreme \n outliers', xy=(0.95, -16.5), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)


plt.show()

"""##7.3 Lưu tệp dữ liệu đã sử dụng kỹ thuật SMOTE"""

from google.colab import drive
drive.mount('drive')

# Dự liệu sau khi làm sạch bằng undersamping, dùng để train.
new_df.to_csv('/content/drive/My Drive/Colab Notebooks/Graduation Thesis/Data/data_credit_smote.csv')
print('=====DONE=====')

"""##7.4 Thuật toán Random forest và SMOTE"""

X = data_smote.drop('Class', axis=1)
y = data_smote['Class']

print('Số  lượng giao dịch trên từng nhãn:\n', y.value_counts())

# Tách dữ liệu thành train set và test set
# Chúng tôi sẽ sử dụng xác thực chéo trên tập huấn luyện để điều chỉnh các tham số, sau đó kiểm tra dữ liệu chưa xem
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42)

print('Số  lượng giao dịch trên từng nhãn trên dữ liệu huấn luyện:\n')
print(y_train.value_counts())
print('Số  lượng giao dịch trên từng nhãn trên dữ liệu kiểm thử:\n')
print(y_test.value_counts())

# Mô hình
model = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    undersample_Xtrain, undersample_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    undersample_ytrain, undersample_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(undersample_Xtrain, undersample_ytrain)
    y_pre_train = model.predict(undersample_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(undersample_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(undersample_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(undersample_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 84575 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 29 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 6 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 77782 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

##7.5 Thuật toán XGBoost và SMOTE
"""

# Mô hình
model = XGBClassifier(objective="binary:logistic", random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    undersample_Xtrain, undersample_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    undersample_ytrain, undersample_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(undersample_Xtrain, undersample_ytrain)
    y_pre_train = model.predict(undersample_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(undersample_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(undersample_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(undersample_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 83601 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 1003 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 2950 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 74838 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

#8.Dữ liệu thuần
"""

X = data_raw.drop('Class', axis=1)
y = data_raw['Class']

print('Số  lượng giao dịch trên từng nhãn:\n', y.value_counts())

# Tách dữ liệu thành train set và test set
# Chúng tôi sẽ sử dụng xác thực chéo trên tập huấn luyện để điều chỉnh các tham số, sau đó kiểm tra dữ liệu chưa xem
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42)

print('Số  lượng giao dịch trên từng nhãn trên dữ liệu huấn luyện:\n')
print(y_train.value_counts())
print('Số  lượng giao dịch trên từng nhãn trên dữ liệu kiểm thử:\n')
print(y_test.value_counts())

"""##8.1 Thuật toán Random Forest và dữ liệu thuần"""

# Mô hình
model = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    undersample_Xtrain, undersample_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    undersample_ytrain, undersample_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(undersample_Xtrain, undersample_ytrain)
    y_pre_train = model.predict(undersample_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(undersample_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(undersample_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(undersample_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 85287 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 7 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 28 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 119 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

##8.2 Thuật toán XGBoost và dữ liệu thuần
"""

# Mô hình
model = XGBClassifier(objective="binary:logistic", random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    undersample_Xtrain, undersample_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    undersample_ytrain, undersample_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(undersample_Xtrain, undersample_ytrain)
    y_pre_train = model.predict(undersample_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(undersample_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(undersample_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(undersample_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 85283 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 11 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 30 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 118 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

#9.Dữ liệu với SMOTE + ENN
"""

X = data_raw.drop('Class', axis=1)
y = data_raw['Class']

print("Trước SMOTE + ENN, số lượng giao dịch gian lận là: {}".format(sum(y==1)))
print("Trước SMOTE + ENN, số lượng giao dịch không gian lận là: {} \n".format(sum(y==0)))

sm = SMOTEENN(random_state=42)
X_res, y_res = sm.fit_sample(X, y.ravel())

print('Sau SMOTE + ENN, Kích thước của dữ liệu train_X: {}'.format(X_res.shape))
print('Sau SMOTE + ENN, Kích thước của dữ liệu train_y: {} \n'.format(y_res.shape))

print("Sau SMOTE + ENN, Số lượng giao dịch gian lận là: {}".format(sum(y_res==1)))
print("Sau SMOTE + ENN, Số lượng giao dịch không gian lận là: {}".format(sum(y_res==0)))

# Convert 'ndarray' to 'DataFrame'
X_res = pd.DataFrame(X_res, columns=X.columns)
y_res = pd.DataFrame(y_res, columns=['Class'])
print(X_res.shape)
print(y_res.shape)

data_smote_enn = pd.concat([X_res, y_res], axis=1)

print(data_smote_enn.shape)

"""##9.1 Ma trận tương quan"""

plt.figure(figsize=(22,10))
# Dữ bộ dữ liệu OverSampling
smote_enn_corr = data_smote_enn.corr()
ax = sns.heatmap(
    smote_enn_corr, cmap='coolwarm_r', annot_kws={'size':20},
    xticklabels=smote_enn_corr.columns, yticklabels=smote_enn_corr.columns)
plt.show()

"""##9.2 Phát hiện bất thường (Anomaly Detection)"""

f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))

v14_fraud_dist = data_smote_enn['V14'].loc[data_smote_enn['Class'] == 1].values
sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')
ax1.set_title('Phân phối thuộc tính V14 \n (Giao dịch gian lận)', fontsize=14)

v12_fraud_dist = data_smote_enn['V12'].loc[data_smote_enn['Class'] == 1].values
sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')
ax2.set_title('Phân phối thuộc tính V12 \n (Giao dịch gian lận)', fontsize=14)


v10_fraud_dist = data_smote_enn['V10'].loc[data_smote_enn['Class'] == 1].values
sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')
ax3.set_title('Phân phối thuộc tính V10 \n (Giao dịch gian lận)', fontsize=14)

plt.show()

v14_fraud_dist# # -----> V14 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v14_fraud = data_smote_enn['V14'].loc[data_smote_enn['Class'] == 1].values
q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)
print('Bách phân bị 25: {} | Bách phân vị 75: {}'.format(q25, q75))
v14_iqr = q75 - q25
print('iqr: {}'.format(v14_iqr))

v14_cut_off = v14_iqr * 1.5
v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off
print('V14 Giá trị biên dưới: {}'.format(v14_lower))
print('V14 Giá trị biên trên: {}'.format(v14_upper))

outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]
print('Tính năng V14 có số giá trị ngoại vi là: {}'.format(len(outliers)))

data_smote_enn = data_smote_enn.drop(data_smote_enn[(data_smote_enn['V14'] > v14_upper) | (data_smote_enn['V14'] < v14_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(data_smote_enn)))
print('----' * 44)

# -----> V12 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v12_fraud = data_smote_enn['V12'].loc[data_smote_enn['Class'] == 1].values
q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)
v12_iqr = q75 - q25

v12_cut_off = v12_iqr * 1.5
v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off
print('V12 Giá trị biên dưới: {}'.format(v12_lower))
print('V12 Giá trị biên trên: {}'.format(v12_upper))
outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]
print('Tính năng V12 có số giá trị ngoại vi là: {}'.format(len(outliers)))

data_smote_enn = data_smote_enn.drop(data_smote_enn[(data_smote_enn['V12'] > v12_upper) | (data_smote_enn['V12'] < v12_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(data_smote_enn)))
print('----' * 44)


# V10 xóa giá trị ngoại vi khỏi các giao dịch gian lận
v10_fraud = data_smote_enn['V10'].loc[data_smote_enn['Class'] == 1].values
q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)
v10_iqr = q75 - q25

v10_cut_off = v10_iqr * 1.5
v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off
print('V10 Giá trị biên dưới: {}'.format(v10_lower))
print('V10 Giá trị biên trên: {}'.format(v10_upper))
outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]
print('Tính năng V10 có số giá trị ngoại vi là: {}'.format(len(outliers)))
data_smote_enn = data_smote_enn.drop(data_smote_enn[(data_smote_enn['V10'] > v10_upper) | (data_smote_enn['V10'] < v10_lower)].index)
print('Số lượng dữ liệu còn lại sau khi loại bỏ các giá trị ngoại vi: {}'.format(len(data_smote_enn)))

f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))

colors = ['#B3F9C5', '#f9c5b3']
# Boxplots sau khi bỏ các giá trị ngoại vi
# Thuộc tính V14
sns.boxplot(x="Class", y="V14", data=data_smote_enn,ax=ax1, palette=colors)
ax1.set_title("V14 Feature \n Reduction of outliers", fontsize=14)
ax1.annotate('Fewer extreme \n outliers', xy=(0.98, -17.5), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)

# Thuộc tính 12
sns.boxplot(x="Class", y="V12", data=data_smote_enn, ax=ax2, palette=colors)
ax2.set_title("V12 Feature \n Reduction of outliers", fontsize=14)
ax2.annotate('Fewer extreme \n outliers', xy=(0.98, -17.3), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)

# Thuộc tính V10
sns.boxplot(x="Class", y="V10", data=data_smote_enn, ax=ax3, palette=colors)
ax3.set_title("V10 Feature \n Reduction of outliers", fontsize=14)
ax3.annotate('Fewer extreme \n outliers', xy=(0.95, -16.5), xytext=(0, -12),
            arrowprops=dict(facecolor='black'),
            fontsize=14)


plt.show()

"""##9.3 Lưu tệp dữ liệu đã sử dụng kỹ thuật SMOTE"""

from google.colab import drive
drive.mount('drive')

# Dự liệu sau khi làm sạch bằng undersamping, dùng để train.
data_smote_enn.to_csv('/content/drive/My Drive/Colab Notebooks/Graduation Thesis/Data/data_credit_smote_enn.csv')
print('=====DONE=====')

"""##9.4 Thuật toán Random Forest và SMOTE + ENN"""

X = data_smote_enn.drop('Class', axis=1)
y = data_smote_enn['Class']

print('Số  lượng giao dịch trên từng nhãn:\n', y.value_counts())

# Tách dữ liệu thành train set và test set
# Chúng tôi sẽ sử dụng xác thực chéo trên tập huấn luyện để điều chỉnh các tham số, sau đó kiểm tra dữ liệu chưa xem
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42)

print('Số  lượng giao dịch trên từng nhãn trên dữ liệu huấn luyện:\n')
print(y_train.value_counts())
print('Số  lượng giao dịch trên từng nhãn trên dữ liệu kiểm thử:\n')
print(y_test.value_counts())

# Mô hình
model = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    smote_enn_Xtrain, smote_enn_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    smote_enn_ytrain, smote_enn_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(smote_enn_Xtrain, smote_enn_ytrain)
    y_pre_train = model.predict(smote_enn_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(smote_enn_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(smote_enn_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(smote_enn_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 78779 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 30 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 5 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 76631 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận

##9.2 Thuật toán XGBoost và SMOTE + ENN
"""

# Mô hình
model = XGBClassifier(objective="binary:logistic", random_state=42)

# Danh sách confusion matrix của từng lần KFold
conf_matrix_list_of_arrays = []
conf_matrix_list_of_arrays_test_set = []

# Danh sách độ chính xác của từng lần KFold trên dữ liệu test
accuracy_train_set = []
accuracy_test_set = []
f1_score_train = []
f1_score_test = []

# Cross Vallidation: KFold = 3
cv = StratifiedKFold(n_splits=3, random_state=None, shuffle=True)
cv.get_n_splits(X_train, y_train)

# List time train
times_train = []

for train_index, test_index in cv.split(X_train, y_train):
    smote_enn_Xtrain, smote_enn_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]
    smote_enn_ytrain, smote_enn_ytest = y_train.iloc[train_index], y_train.iloc[test_index]

    # Thời gian bắt đầu train
    start_time = timer()

    model.fit(smote_enn_Xtrain, smote_enn_ytrain)
    y_pre_train = model.predict(smote_enn_Xtest)
    y_pre_test = model.predict(X_test)

    # Print the accuracy
    accuracy_train_set.append(
        accuracy_score(smote_enn_ytest, y_pre_train, normalize=True)*100)
    accuracy_test_set.append(
        accuracy_score(y_test, y_pre_test, normalize=True)*100)

    # Confusion matrix
    conf_matrix = confusion_matrix(smote_enn_ytest, y_pre_train)
    conf_matrix_list_of_arrays.append(conf_matrix)
    conf_matrix_2 = confusion_matrix(y_test, y_pre_test)
    conf_matrix_list_of_arrays_test_set.append(conf_matrix_2)

    # F1 score
    f1_score_train.append(f1_score(smote_enn_ytest, y_pre_train))
    f1_score_test.append(f1_score(y_test, y_pre_test))

    # Thời gian kết thúc train
    end_time = timer()
    # Tính thời gian thực hiện
    time_train = (end_time - start_time)
    times_train.append(time_train)

# Độ chính xác của mô hình
print('Độ chính xác của mô hình trên dữ liệu train: ', accuracy_train_set)
print('Độ chính xác của mô hình trên dữ liệu test: ', accuracy_test_set)

print('Độ chính xác trung bình dữ liệu train:', np.mean(accuracy_train_set))
print('Độ chính xác trung bình dữ liệu test:', np.mean(accuracy_test_set))

print('F1 score trung bình trên dữ liệu train:', np.mean(f1_score_train))
print('F1 score trung bình trên dữ liệu test:', np.mean(f1_score_test))

print('Thời gian thực hiện: ', times_train)

# Confusion matrix: dữ liệu test
actual_cm_test_set = np.mean(conf_matrix_list_of_arrays_test_set, axis=0).astype(int)
labels = ['Not Fraud', 'Fraud']
plot_confusion_matrix(actual_cm_test_set, labels, title="Confusion Matrix \nTrên dữ liệu kiểm thử", cmap=plt.cm.Greens)

"""###Kết quả

- Có 78352 giao dịch không gian lận được dự đoán đúng là giao dịch không gian lận
- 456 lần giao dịch không gian lận được dự sai đoán là giao dịch gian lận
- 1395 lần giao dịch gian lận được dự đoán sai là giao dịch không gian lận
- 75241 lần giao dịch gian lận được dự đoán chính xác là giao dịch gian lận
"""